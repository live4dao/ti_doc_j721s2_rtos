<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>TI Deep Learning Product User Guide: TFLite Runtime</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ti_logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TI Deep Learning Product User Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_tidl_osr_tflrt_tidl.html','');});
/* @license-end */
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">TFLite Runtime </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#tflrt_tidl_Intro">Introduction</a></li>
<li class="level1"><a href="#tflrt_work_flow">TFLite runtime based user work flow</a></li>
<li class="level1"><a href="#tflrt_tidl_Compilation">Model Compilation on PC</a></li>
<li class="level1"><a href="#tflrt_tidl_run">Model Inference on EVM</a><ul><li class="level2"><a href="#tflrt_python_demo">Python API based inference</a></li>
</ul>
</li>
<li class="level1"><a href="#tflrt_user_options">User options</a><ul><li class="level2"><a href="#tflrt_user_options_req">Required</a></li>
<li class="level2"><a href="#tflrt_user_options_optional">Optional</a></li>
</ul>
</li>
<li class="level1"><a href="#tflrt_C_demo">C API based inference</a></li>
<li class="level1"><a href="#tflrt_tidl_known_issues">Known Issues/Limitations</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="tflrt_tidl_Intro"></a>
Introduction</h1>
<p>The Processor SDK implements TIDL offload support using the TFlite Delegates <a href="https://www.tensorflow.org/lite/performance/delegates">TFLite Delgate</a> runtime</p>
<p>This heterogeneous execution enables:</p><ol type="1">
<li>TFlite runtime as the top level inference API for user applications</li>
<li>Offloading subgraphs to C7x/MMA for accelerated execution with TIDL</li>
<li>Runs optimized code on ARM core for layers that are not supported by TIDL</li>
</ol>
<h1><a class="anchor" id="tflrt_work_flow"></a>
TFLite runtime based user work flow</h1>
<p>The diagram below illustrates the TFLite based work flow. The User needs to run the model compilation (sub-graph(s) creation and quantization) on PC and the generated artifacts can be used for inference on the device.</p>
<div class="image">
<img src="tflrt_work_flow.png" alt="tflrt_work_flow.png"/>
<div class="caption">
TFLite runtime based user work flow</div></div>
 <h1><a class="anchor" id="tflrt_tidl_Compilation"></a>
Model Compilation on PC</h1>
<div class="image">
<img src="osrt_compile_steps.png" alt="osrt_compile_steps.png"/>
<div class="caption">
OSRT Compile Steps</div></div>
<p> The Processor SDK package includes all the required python packages for runtime support.</p>
<p>Pre-requisite : PSDK RA should be installed on the Host Ubuntu 18.04 machine and able to run pre-built demos on EVM.</p>
<p>Following steps need to be followed : (Note - All below scripts to be run from ${PSDKRA_PATH}/tidl_xx_xx_xx_xx/ti_dl/test/tflrt/ folder)</p>
<ol type="1">
<li>Prepare the Environment for the Model compilation <div class="fragment"><div class="line">source prepare_model_compliation_env.sh</div></div><!-- fragment --> This script needs to be executed only once when user opens a new terminal. It performs the below operations. User also can perform these steps manually by following the scripts.<ul>
<li>Installs all the python dependent packages like tensorflow lite runtime, numPy, Python image library (Pillow) etc. If user has a conflicting package version because of other installations, we recommend to create conda environment with python 3.6.x and run these scripts.</li>
<li>Download the models used by the OOB scripts if not available in the file system</li>
<li>Sets the environment variables required by the script e.g. path to tools and shared libraries</li>
<li>Checks if all the TIDL required tools are available in tools path <blockquote class="doxtable">
<p><b>Note</b><br />
If you observe any issue in pip. Run below command to update pip </p><div class="fragment"><div class="line">python -m pip install --upgrade pip</div></div><!-- fragment --> </blockquote>
</li>
</ul>
</li>
<li>Run for model compilation – This step generates artifacts needed for inference in the tflrt-artifacts folder. Each subgraph is identified in the artifacts using the tensor index of its output in the model <div class="fragment"><div class="line">python3 tflrt_delegate.py -c</div></div><!-- fragment --></li>
<li>Run Inference on PC - Optionally user can test the inference in host emulation mode and check the output; the output images will be saved in the corresponding specified artifacts folder <div class="fragment"><div class="line">python3 tflrt_delegate.py</div></div><!-- fragment --></li>
<li>Run Inference on PC without offload - Optionally user can test the inference in host emulation mode without using any delegation to TI Delegate <div class="fragment"><div class="line">python3 tflrt_delegate.py -d</div></div><!-- fragment --></li>
</ol>
<h1><a class="anchor" id="tflrt_tidl_run"></a>
Model Inference on EVM</h1>
<p>The artifacts generated by python scripts in the above section can be inferred using either python or C/C++ APIs. The following steps are for running inference using python API. Refer <a href="#tflrt_C_demo">Link</a> for usage of C APIs for the same</p>
<h2><a class="anchor" id="tflrt_python_demo"></a>
Python API based inference</h2>
<div class="image">
<img src="osrt_run_steps.png" alt="osrt_run_steps.png"/>
<div class="caption">
OSRT Run Steps</div></div>
<ol type="1">
<li>Copy the “${PSDKR_PATH}/tidl_xx_xx_xx_xx/ti_dl/test/” folder to the file system where the EVM is running Linux (SD card or NFS mount). This has all the OOB scripts and artifacts.</li>
<li>"cd tflrt"</li>
<li>"LD_LIBRARY_PATH=/usr/lib python3 tflrt_delegate.py" - Run the inference on the EVM and check the results, performance etc.</li>
</ol>
<p>Note : These scripts are only for basic functionally testing and performance check. Accuracy of the models can be benchmarked using the python module released here <a href="https://git.ti.com/cgit/jacinto-ai/jacinto-ai-benchmark/">edgeai-benchmark</a></p>
<p>We also have IPython Notebooks for running inference on EVM. More details on this can be found here <a class="el" href="md_tidl_notebook.html">Link</a></p>
<h1><a class="anchor" id="tflrt_user_options"></a>
User options</h1>
<p>An example call to interpreter from the python interface using delegate mechanism: </p><pre class="fragment">interpreter = tflite.Interpreter(model_path='path_to_model', \
                    experimental_delegates=[tflite.load_delegate('libtidl_tfl_delegate.so.1.0', delegate_options)])
</pre><p>'delegate_options' in the interpreter call comprise of the below options (required and optional):</p>
<h2><a class="anchor" id="tflrt_user_options_req"></a>
Required</h2>
<p>The following options need to be specified by user while creating TFLite interpreter:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Name  </th><th class="markdownTableHeadLeft">Value   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">tidl_tools_path  </td><td class="markdownTableBodyLeft">to be set to ${PSDKRA_PATH}/tidl_xx_xx_xx_xx/tidl_tools/ - Path from where to pick TIDL related tools   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">artifacts_folder  </td><td class="markdownTableBodyLeft">folder where user intends to store all the compilation artifacts   </td></tr>
</table>
<h2><a class="anchor" id="tflrt_user_options_optional"></a>
Optional</h2>
<p>The following options are set to default values, to be specified if modification needed by user. Below optional arguments are specific to model compilation and not applicable to inference except the 'debug_level'</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Name  </th><th class="markdownTableHeadLeft">Description  </th><th class="markdownTableHeadCenter">Default values   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">platform  </td><td class="markdownTableBodyLeft">"J7"  </td><td class="markdownTableBodyCenter">"J7"   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">version  </td><td class="markdownTableBodyLeft">TIDL version - open source runtimes supported from version 7.2 onwards  </td><td class="markdownTableBodyCenter">(7,3)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">tensor_bits  </td><td class="markdownTableBodyLeft">Number of bits for TIDL tensor and weights - 8/16  </td><td class="markdownTableBodyCenter">8   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">debug_level  </td><td class="markdownTableBodyLeft">0 - no debug, 1 - rt debug prints, &gt;=2 - increasing levels of debug and trace dump  </td><td class="markdownTableBodyCenter">0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">max_num_subgraphs  </td><td class="markdownTableBodyLeft">offload up to &lt;num&gt; tidl subgraphs  </td><td class="markdownTableBodyCenter">16   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">deny_list  </td><td class="markdownTableBodyLeft">force disable offload of a particular operator to TIDL [^2]  </td><td class="markdownTableBodyCenter">"" - Empty list   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">accuracy_level  </td><td class="markdownTableBodyLeft">0 - basic calibration, 1 - higher accuracy(advanced bias calibration), 9 - user defined [^3]  </td><td class="markdownTableBodyCenter">1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:calibration_frames  </td><td class="markdownTableBodyLeft">Number of frames to be used for calibration - min 10 frames recommended  </td><td class="markdownTableBodyCenter">20   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:calibration_iterations  </td><td class="markdownTableBodyLeft">Number of bias calibration iterations [^1]  </td><td class="markdownTableBodyCenter">50   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:output_feature_16bit_names_list  </td><td class="markdownTableBodyLeft">List of names of the layers (comma separated string) as in the original model whose feature/activation output user wants to be in 16 bit [^1] [^4]  </td><td class="markdownTableBodyCenter">""   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:params_16bit_names_list  </td><td class="markdownTableBodyLeft">List of names of the output layers (separated by comma or space or tab) as in the original model whose parameters user wants to be in 16 bit [^1] [^5]  </td><td class="markdownTableBodyCenter">""   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:quantization_scale_type  </td><td class="markdownTableBodyLeft">0 for non-power-of-2, 1 for power-of-2  </td><td class="markdownTableBodyCenter">0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:high_resolution_optimization  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable  </td><td class="markdownTableBodyCenter">0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:pre_batchnorm_fold  </td><td class="markdownTableBodyLeft">Fold batchnorm layer into following convolution layer, 0 for disable, 1 for enable  </td><td class="markdownTableBodyCenter">1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:add_data_convert_ops  </td><td class="markdownTableBodyLeft">Adds the Input and Output format conversions to Model and performs the same in DSP instead of ARM. This is currently a experimental feature.  </td><td class="markdownTableBodyCenter">0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">object_detection:confidence_threshold  </td><td class="markdownTableBodyLeft">Override "nms_score_threshold" parameter threshold in tflite detection post processing layer  </td><td class="markdownTableBodyCenter">Read from model   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">object_detection:nms_threshold  </td><td class="markdownTableBodyLeft">Override "nms_iou_threshold" parameter threshold in tflite detection post processing layer  </td><td class="markdownTableBodyCenter">Read from model   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">object_detection:top_k  </td><td class="markdownTableBodyLeft">Override "detections_per_class" parameter threshold in tflite detection post processing layer  </td><td class="markdownTableBodyCenter">Read from model   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">object_detection:keep_top_k  </td><td class="markdownTableBodyLeft">Override "max_detections" parameter threshold in tflite detection post processing layer  </td><td class="markdownTableBodyCenter">Read from model   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">ti_internal_nc_flag  </td><td class="markdownTableBodyLeft">internal use only  </td><td class="markdownTableBodyCenter">-   </td></tr>
</table>
<p>Below options will be overwritten only if accuracy_level = 9, else will be discarded. For accuracy level 9, specified options will be overwritten, rest will be set to default values. For accuracy_level = 0/1, these are preset internally.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Name  </th><th class="markdownTableHeadLeft">Description  </th><th class="markdownTableHeadCenter">Default values   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:activation_clipping  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable [^1]  </td><td class="markdownTableBodyCenter">1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:weight_clipping  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable [^1]  </td><td class="markdownTableBodyCenter">1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">advanced_options:bias_calibration  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable [^1]  </td><td class="markdownTableBodyCenter">1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options:channel_wise_quantization  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable [^1]  </td><td class="markdownTableBodyCenter">0   </td></tr>
</table>
<p>[^1]: Advanced calibration can help improve 8-bit quantization. Please see <a href="md_tidl_fsg_quantization.html#did_tidl_quantization_1">TIDL Quantization</a> for details. <br />
[^2]: Denylist is a string of comma separated numbers which represent the operators as identified in tflite builtin ops. Please refer <a href="https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/builtin_ops.h">Tflite builtin ops</a> , e.g. deny_list = "1, 2" to deny offloading 'AveragePool2d' and 'Concatenation' operators to TIDL. <br />
[^3]: Advanced calibration options can be specified by setting accuracy_level = 9. <br />
[^4]: Note that if for a given layer feature/activations is in 16 bit then parameters will automatically become 16 bit and user need not specify them as part of "advanced_options:params_16bit_names_list". Example format - "conv1_2, fire9/concat_1" <br />
[^5]: This is not the name of the parameter of the layer but is expected to be the output name of the layer. Note that, if a given layers feature/activations is in 16 bit then parameters will automatically become 16 bit even if its not part of this list <br />
</p>
<h1><a class="anchor" id="tflrt_C_demo"></a>
C API based inference</h1>
<p>Pre-requisite: Compiled artifacts stored in artifacts folder as specified in step 2 of 'Model Compilation on PC' above</p>
<p>Following steps are needed to run C API based demo for tflite runtime: </p><pre class="fragment">cd ${PSDKRA_PATH}
git clone --single-branch -b r2.4 https://github.com/tensorflow/tensorflow.git

cd tensorflow
git checkout 582c8d236cb079023657287c318ff26adb239002
git am ../tidl_j7_xx_xx_xx_xx/ti_dl/tfl_delegate/0001-tflite-interpreter-add-support-for-custom-data.patch
./tensorflow/lite/tools/make/download_dependencies.sh

cd ..
export PSDK_INSTALL_PATH=$(pwd)

cd targetfs/usr/lib/
ln -s libtbb.so.2 libtbb.so
ln -s libtiff.so.5 libtiff.so
ln -s libwebp.so.7 libwebp.so
ln -s libopencv_highgui.so.4.1 libopencv_highgui.so
ln -s libopencv_imgcodecs.so.4.1 libopencv_imgcodecs.so
ln -s libopencv_core.so.4.1.0  libopencv_core.so
ln -s libopencv_imgproc.so.4.1.0 libopencv_imgproc.so  

cd tidl_j7_xx_xx_xx_xx
make demos DIRECTORIES=tfl

# To run on EVM:

Mount ${PSDKR_PATH} on EVM
export LD_LIBRARY_PATH=/usr/lib
cd ${PSDKR_PATH}/tidl_j7_xx_xx_xx_xx/ti_dl/demos/out/J7/A72/LINUX/release/

./tidl_tfl_classification.out -m ../../../../../../test/testvecs/models/public/tflite/mobilenet_v1_1.0_224.tflite -l ../../../../../../test/testvecs/input/labels.txt -i ../../../../../../test/testvecs/input/airshow.bmp -a 1
</pre><h1><a class="anchor" id="tflrt_tidl_known_issues"></a>
Known Issues/Limitations</h1>
<ul>
<li>Some amount of accuracy degradation is seen in quant models on multiple subgraph creation, this will be fixed in the next release.</li>
<li>tidl_denylist feature has not been extensively tested for object detection networks in this release.</li>
<li>Some networks with odd input resolutions e.g. 513X513 are seen to get stuck on target with the use of 8 bit mode, please use 16 bit mode for these networks. This issue will be fixed in next release. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
