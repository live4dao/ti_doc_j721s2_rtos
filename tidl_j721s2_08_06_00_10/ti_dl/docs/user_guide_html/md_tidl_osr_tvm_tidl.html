<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>TI Deep Learning Product User Guide: TVM/Neo-AI-DLR</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ti_logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TI Deep Learning Product User Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_tidl_osr_tvm_tidl.html','');});
/* @license-end */
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">TVM/Neo-AI-DLR </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#tvm_tidl_Intro">Introduction</a></li>
<li class="level1"><a href="#tvmrt_work_flow">TVM/NEO-AI-DLR based user work flow</a></li>
<li class="level1"><a href="#tvmrt_tidl_Compilation">Model Compilation on PC</a></li>
<li class="level1"><a href="#tvmrt_tidl_run">Model Inference on EVM</a><ul><li class="level2"><a href="#tvmrt_python_demo">Python API based inference</a></li>
</ul>
</li>
<li class="level1"><a href="#tvmrt_tidl_user_options">TIDL specific lines in compilation script</a></li>
<li class="level1"><a href="#tvmrt_c_demo">C API based inference</a></li>
<li class="level1"><a href="#tvmrt_advanced_users">Options for Advanced Users</a><ul><li class="level2"><a href="#tvmrt_advanced_users_dm">Deployable module</a></li>
<li class="level2"><a href="#tvmrt_advanced_users_artifacts">Other compilation artifacts</a></li>
<li class="level2"><a href="#tvmrt_advanced_users_debugging">Debugging</a><ul><li class="level3"><a href="#tvmrt_advanced_users_debug_trace">Comparing TIDL per layer output with TVM per layer output</a></li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#tvm_tidl_known_osrt_versions">Known Compatible Open Source Runtime Versions</a></li>
<li class="level1"><a href="#tvm_tidl_known_issues">Known Issues/Limitations</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="tvm_tidl_Intro"></a>
Introduction</h1>
<p>The Processor SDK implements TIDL offload support using the <a href="https://tvm.apache.org/">TVM</a> runtime and <a href="https://github.com/neo-ai/neo-ai-dlr">Neo-AI-DLR</a> runtime. This heterogeneous execution enables:</p><ol type="1">
<li>TVM/Neo-AI-DLR as the top level inference API for user applications</li>
<li>Offloading subgraphs to C7x/MMA for accelerated execution with TIDL</li>
<li>Generating code and running on the ARM core for layers that are not supported by TIDL</li>
</ol>
<p>Neo-AI-DLR is an open source common runtime for machine learning models compiled by AWS SageMaker Neo, TVM, or Treelite. For the Processor SDK, we focus on models compiled by TVM. For these models, the Neo-AI-DLR runtime can be considered as a wrapper around the TVM runtime.</p>
<p>The following sections describe the details for compiling and deploying machine learning models for TVM/Neo-AI-DLR + TIDL heterogeneous execution.</p>
<h1><a class="anchor" id="tvmrt_work_flow"></a>
TVM/NEO-AI-DLR based user work flow</h1>
<p>The diagram below illustrates TVM/NEO-AI-DLR based work flow. The user needs to run the model compilation (sub-graph(s) creation and quantization) on PC and the generated artifacts can be used for inference on the device.</p>
<div class="image">
<img src="tvmrt_work_flow.png" alt="tvmrt_work_flow.png"/>
<div class="caption">
TVM/NEO-AI-DLR based user work flow</div></div>
 <h1><a class="anchor" id="tvmrt_tidl_Compilation"></a>
Model Compilation on PC</h1>
<div class="image">
<img src="osrt_compile_steps.png" alt="osrt_compile_steps.png"/>
<div class="caption">
OSRT Compile Steps</div></div>
<p> The Processor SDK package includes all the required python packages for runtime support.</p>
<p>Pre-requisite : PSDK RA should be installed on the Host Ubuntu 18.04 machine and able to run pre-built demos on EVM.</p>
<p>Following steps need to be followed : (Note - All below scripts to be run from ${PSDKRA_PATH}/tidl_xx_xx_xx_xx/ti_dl/test/tvm-dlr/ folder)</p>
<ol type="1">
<li>Prepare the Environment for the Model compilation <div class="fragment"><div class="line">source prepare_model_compliation_env.sh</div></div><!-- fragment --> This script needs to be executed only once when user opens a new terminal. It performs the below operations. User also can perform these steps manually by following the scripts.<ul>
<li>Downloads and Installs all the python dependent packages like TVM, DLR, numPy, Python image library (Pillow) etc. If user has a conflicting package version because of other installations, we recommend to create conda environment with python 3.6.x and run these scripts.</li>
<li>Download the models used by the OOB scripts if not available in the file system</li>
<li>Sets the environment variables required by the script e.g. path to tools and shared libraries</li>
<li>Checks if all the TIDL required tools are available in tools path <blockquote class="doxtable">
<p><b>Note</b><br />
This scripts downalod the TVM and DLR python packages from latest PSDK release in the ti.com If you are using diffrent version of SDK (Example RC versions), then please update links for these two python whl files in the download_models.py - 'dlr-1.4.0-py3-none-any.whl' : {'url':'<a href="http://swubn03.india.englab.ti.com/webgen/publish/PROCESSOR-SDK-LINUX-J721E/07_02_00_05/exports//dlr-1.4.0-py3-none-any.whl',">http://swubn03.india.englab.ti.com/webgen/publish/PROCESSOR-SDK-LINUX-J721E/07_02_00_05/exports//dlr-1.4.0-py3-none-any.whl',</a> 'dir':'./'} If you observe any issue in pip. Run below command to update pip </p><div class="fragment"><div class="line">python -m pip install --upgrade pip</div></div><!-- fragment --> </blockquote>
</li>
</ul>
</li>
<li>Run for model compilation – This step generates artifacts needed for inference in the artifacts folder. Each subgraph is identified in the artifacts using the tensor index of its output in the mode <div class="fragment"><div class="line">#Model Compilation for EVM</div><div class="line">python3 tvm-compilation-tflite-example.py</div><div class="line">python3 tvm-compilation-onnx-example.py</div></div><!-- fragment --> <div class="fragment"><div class="line">#Model Compilation for PC</div><div class="line">python3 tvm-compilation-tflite-example.py --pc-inference</div><div class="line">python3 tvm-compilation-onnx-example.py --pc-inference</div></div><!-- fragment --></li>
<li>Run Inference on PC - Optionally user can test the inference in host emulation mode and check the output in the console <div class="fragment"><div class="line">python3 dlr-inference-example.py</div></div><!-- fragment --></li>
</ol>
<h1><a class="anchor" id="tvmrt_tidl_run"></a>
Model Inference on EVM</h1>
<p>The artifacts generated by python scripts in the above section can be inferred using either python or C/C++ APIs. The following steps are for running inference using python API. Refer <a href="#tvmrt_C_demo">Link</a> for usage of C APIs for the same</p>
<h2><a class="anchor" id="tvmrt_python_demo"></a>
Python API based inference</h2>
<div class="image">
<img src="osrt_run_steps.png" alt="osrt_run_steps.png"/>
<div class="caption">
OSRT Run Steps</div></div>
<ol type="1">
<li>Copy the “${PSDKR_PATH}/tidl_xx_xx_xx_xx/ti_dl/test/” folder to the file system where the EVM is running Linux (SD card or NFS mount). This has all the OOB scripts and artifacts.</li>
<li>"cd tvm-dlr"</li>
<li>“LD_LIBRARY_PATH=/usr/lib python3 dlr-inference-example.py” - Run the inference on the EVM and check the results, performance etc.</li>
</ol>
<p>Note : These scripts are only for basic functionally testing and performance check. Accuracy of the models can be benchmarked using the python module released here <a href="https://git.ti.com/cgit/jacinto-ai/jacinto-ai-benchmark/">edgeai-benchmark</a></p>
<p>We also have IPython Notebooks for running inference on EVM. More details on this can be found here <a class="el" href="md_tidl_notebook.html">Link</a></p>
<h1><a class="anchor" id="tvmrt_tidl_user_options"></a>
TIDL specific lines in compilation script</h1>
<p>There are only 4 lines that are specific to TIDL offload in TVM+TIDL compilation scripts. The rest of the script is no different from a regular TVM compilation script without TIDL offload. </p><pre class="fragment">tidl_compiler = tidl.TIDLCompiler(platform="J7", version="7.3",
                                  tidl_tools_path=get_tidl_tools_path(),
                                  artifacts_folder=tidl_artifacts_folder,
                                  tensor_bits=8,
                                  max_num_subgraphs=max_num_subgraphs,
                                  deny_list=args.denylist,
                                  accuracy_level=1,
                                  advanced_options={'calibration_iterations':10}
                                 )
</pre><p>We first instantiate a TIDLCompiler object. The parameters are explained in the following table.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Name/Position  </th><th class="markdownTableHeadLeft">Value   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">platform  </td><td class="markdownTableBodyLeft">"J7"   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">version  </td><td class="markdownTableBodyLeft">"7.3"   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">tidl_tools_path  </td><td class="markdownTableBodyLeft">set to environment variable TIDL_TOOLS_PATH, usually psdk_rtos_install/tidl_xx_yy_zz_ww/ti_dl/tidl_tools   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">artifacts_folder  </td><td class="markdownTableBodyLeft">where to store deployable module   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><b>Optional Parameters</b>  </td><td class="markdownTableBodyLeft"></td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">tensor_bits  </td><td class="markdownTableBodyLeft">8 or 16 for import TIDL tensor and weights, default is 8   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">debug_level  </td><td class="markdownTableBodyLeft">0, 1, 2, 3, 4 for various debug info, default is 0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">max_num_subgraphs  </td><td class="markdownTableBodyLeft">offload up to &lt;num&gt; tidl subgraphs, default is 16   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">deny_list  </td><td class="markdownTableBodyLeft">deny TVM relay ops for TIDL offloading, comma separated string, default is ""   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">accuracy_level  </td><td class="markdownTableBodyLeft">0 for simple calibration, 1 for advanced bias calibration, 9 for user defined, default is 1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">ti_internal_nc_flag  </td><td class="markdownTableBodyLeft">internal use only , default is 0x641   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">advanced_options  </td><td class="markdownTableBodyLeft">a dictionary to overwrite default calibration options, default is {}   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><b>advanced_options Keys</b>  </td><td class="markdownTableBodyLeft">(if not specified, defaults are used)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">'calibration_iterations'  </td><td class="markdownTableBodyLeft">number of calibration iterations , default is 50   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">'quantization_scale_type'  </td><td class="markdownTableBodyLeft">0 for non-power-of-2, 1 for power-of-2, default is 0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">'high_resolution_optimization'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable, default is 0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">'pre_batchnorm_fold'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable, default is 1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">'output_feature_16bit_names_list'  </td><td class="markdownTableBodyLeft">comma separated string, default is ""   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">'params_16bit_names_list'  </td><td class="markdownTableBodyLeft">comma separated string, default is ""   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft"></td><td class="markdownTableBodyLeft">(below are overwritable at accuracy level 9 only)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">'activation_clipping'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">'weight_clipping'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">'bias_calibration'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">'channel_wise_quantization'  </td><td class="markdownTableBodyLeft">0 for disable, 1 for enable   </td></tr>
</table>
<p>Advanced calibration can help improve 8-bit quantization. Please see <a href="md_tidl_fsg_quantization.html#did_tidl_quantization_1">TIDL Quantization</a> for details. Default advanced options are specified in tvm source file, python/tvm/relay/backend/contrib/tidl.py. Please grep for "default_advanced_options". </p><pre class="fragment">mod, status = tidl_compiler.enable(mod_orig, params, model_input_list)
</pre><p>In this step, the original machine learning model/network represented in TVM Relay IR, "mod_orig", goes through the following transformations:</p><ol type="1">
<li>Allowlisting: each Relay Op is examined to see if it can be offloaded to TIDL</li>
<li>Partitioning: Ops that TIDL supports are partitioned into TIDL subgraphs</li>
<li>TIDL importing: TIDL subgraphs are imported from Relay IR into TIDL format</li>
<li>TIDL postprocessing: sample inputs in "model_input_list" are used to calibrate quantization in the TIDL subgraphs.</li>
</ol>
<pre class="fragment">with tidl.build_config(tidl_compiler=tidl_compiler):
    graph, lib, params = relay.build_module.build(mod, target=target, params=params)
</pre><p>In this step, TVM code generation takes place. Inside the TVM codegen, there is a TIDL codegen backend. "tidl.build_config" creates a context and tells the TIDL codegen backend where the artifacts from TIDL importing are. The backend then embeds the artifacts into the "lib". </p><pre class="fragment">tidl.remove_tidl_params(params)
</pre><p>This optional step removes the weights in TIDL subgraphs that have already been imported into the artifacts. Removing them results in a smaller deployable module.</p>
<h1><a class="anchor" id="tvmrt_c_demo"></a>
C API based inference</h1>
<p>NEO-AI-DLR on the EVM/Target supports both Pythin and C-API. This section of describes the usage of C-API</p>
<p>Pre-requisite: Compiled artifacts stored in artifacts folder as specified in step 2 of 'Model Compilation on PC' above.</p>
<p>Following steps are needed to run C API based demo for DLR runtime. Note : This is only an example C API demo, user may need to modify code for using other models and images. </p><pre class="fragment">cd ${PSDKRA_PATH}
export PSDK_INSTALL_PATH=$(pwd)

cd targetfs/usr/lib/
ln -s libtbb.so.2 libtbb.so
ln -s libtiff.so.5 libtiff.so
ln -s libwebp.so.7 libwebp.so
ln -s libopencv_highgui.so.4.1 libopencv_highgui.so
ln -s libopencv_imgcodecs.so.4.1 libopencv_imgcodecs.so
ln -s libopencv_core.so.4.1.0  libopencv_core.so
ln -s libopencv_imgproc.so.4.1.0 libopencv_imgproc.so  

cd ../../../tidl_j7_xx_xx_xx_xx

make demos DIRECTORIES=dlr

# To run on EVM:

Mount ${PSDKR_PATH} on EVM
export LD_LIBRARY_PATH=/usr/lib
cd ${PSDKR_PATH}/tidl_j7_xx_xx_xx_xx/ti_dl/demos/out/J7/A72/LINUX/release/

./tidl_tdlr_classification.out -m ../../../../../../test/testvecs/models/public/tflite/mobilenet_v1_1.0_224.tflite -l ../../../../../../test/testvecs/input/labels.txt -i ../../../../../../test/testvecs/input/airshow.bmp
</pre><h1><a class="anchor" id="tvmrt_advanced_users"></a>
Options for Advanced Users</h1>
<h2><a class="anchor" id="tvmrt_advanced_users_dm"></a>
Deployable module</h2>
<p>The result of compilation is called a "deployable module". It consists of three files:</p><ol type="1">
<li>deploy_graph.json: graph description of the compiled network for execution. In the graph, TIDL subgraphs are nodes with names "tidl_0", "tidl_1", etc.</li>
<li>deploy_lib.so: executable code that runs the nodes in the graph. Code for offloading TIDL subgraphs and imported TIDL artifacts is also embedded in this file.</li>
<li>deploy_param.params: constant weights/parameters for nodes in graph.</li>
</ol>
<p>Taking the output of "tvm-compilation-onnx-example.py" for ONNX MobilenetV2 for example, the deployable module for J7 target is located in "onnx_mobilenetv2/". You can copy this deployable module to the target EVM for execution. Please see the above "Run Model on EVM" section for details. </p><pre class="fragment">onnx_mobilenetv2
|-- deploy_graph.json
|-- deploy_lib.so
|-- deploy_param.params
</pre><h2><a class="anchor" id="tvmrt_advanced_users_artifacts"></a>
Other compilation artifacts</h2>
<p>All other compilation artifacts are stored in the "tempDir" directory under the specified "artifacts_folder". Interested users can look into this directory for TIDL importing details. This directory is for information only, and is not needed for inference/deployment.</p>
<p>One useful file is "relay.gv.svg". It gives a graphical view of the whole network and where the TIDL subgraphs are. You can view it using a browser or other viewer, for example: </p><pre class="fragment">firefox onnx_mobilenetv2/tempDir/relay.gv.svg
</pre><h2><a class="anchor" id="tvmrt_advanced_users_debugging"></a>
Debugging</h2>
<p>You can set the TIDLCompiler parameter "debug_level" to 0, 1, 2, 3, or 4 for detailed internal debug information and progress during TVM compilation. For example the compiler will dump the graph represented in TVM Relay IR, RelayIR to TIDL importing, etc.</p>
<h3><a class="anchor" id="tvmrt_advanced_users_debug_trace"></a>
Comparing TIDL per layer output with TVM per layer output</h3>
<p>When "debug_level" is set to 4, TIDL import will generate the output for each TIDL layer in the imported TIDL subgraph, using calibration inputs. The compilation will also generate corresponding output from running the original model in floating point mode, by compiling and running on the host using TVM. We name the tensors from TIDL quantized calibration execution "tidl_tensor"; we name the corresponding tensors from TVM floating point execution "tvm_tensor". A simple script, <a href="https://github.com/TexasInstruments/tvm/blob/tidl-j7/tests/python/relay/ti_tests/compare_tensors.py">"compare_tensors.py"</a>, is provided to compare these two tensors. </p><pre class="fragment">python3 ./tvm-compilation-onnx-example.py  # with TIDLCompiler(...,debug_level=4)
# python3 ./compare_tensors.py &lt;artifacts_folder&gt; &lt;subgraph_id&gt; &lt;layer_id&gt;
python3 ./compare_tensors.py onnx_mobilentv2 0 1
</pre><h1><a class="anchor" id="tvm_tidl_known_osrt_versions"></a>
Known Compatible Open Source Runtime Versions</h1>
<p>TVM compilation flow can accept multiple network formats in various Open Source Runtime formats. The following are the list of known compataible OSRT python package versions that we have tested with the PSDK TVM compilation flow.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Python Package Name  </th><th class="markdownTableHeadCenter">Version Compatible with PSDK TVM   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">tflite  </td><td class="markdownTableBodyCenter">2.4.0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">onnx  </td><td class="markdownTableBodyCenter">1.9.0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">mxnet  </td><td class="markdownTableBodyCenter">1.7.0.post2   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">gluoncv  </td><td class="markdownTableBodyCenter">0.8.0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">torch  </td><td class="markdownTableBodyCenter">1.7.0   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">torchvision  </td><td class="markdownTableBodyCenter">0.8.1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">tensorflow  </td><td class="markdownTableBodyCenter">1.14.0   </td></tr>
</table>
<h1><a class="anchor" id="tvm_tidl_known_issues"></a>
Known Issues/Limitations</h1>
<ul>
<li>We are observing performance gap between the NEO_AI-DLR runtime and TIDL standalone runtime when entire network is fully offloaded also. Expected to reduce this gap with future releases.</li>
<li>mxnet_mobilenetv3_large - Squeeze and excitation (SE) is currently not supported in TIDL which resulted in creation of many subgraphs. Due to this graph switching overhead is quite high which ultimately nullified acceleration benefit of off-loading subgraphs to C7x-MMA. This may applicable to other networks also when number of subgraphs are more. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
