<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>TI Deep Learning Product User Guide: TIDL-RT Importer</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ti_logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TI Deep Learning Product User Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_tidl_model_import.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">TIDL-RT Importer </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#tidl_model_import_intro">Introduction</a></li>
<li class="level1"><a href="#tidl_model_import_mf">Supported versions of Model Format</a></li>
<li class="level1"><a href="#tidl_model_import_config">TIDL-RT Import Configuration Parameters</a><ul><li class="level2"><a href="#tidl_model_import_basic">Basic Configuration Parameters</a></li>
<li class="level2"><a href="#tidl_model_import_ip">Configuration Parameters for Input Pre-Processing</a></li>
<li class="level2"><a href="#tidl_model_import_path">Configuration Parameters for path of different modules</a></li>
<li class="level2"><a href="#tidl_model_import_infer_quantization">Configuration Parameters related to TIDL-RT inference for quantization</a></li>
<li class="level2"><a href="#tidl_model_import_quantization">Configuration Parameters for quantization</a></li>
<li class="level2"><a href="#tidl_model_import_gc">Configuration Parameters for Graph Compiler</a></li>
<li class="level2"><a href="#tidl_model_import_fc">Configuration Parameters for format conversion</a></li>
</ul>
</li>
<li class="level1"><a href="#tidl_model_import_ex">Examples</a><ul><li class="level2"><a href="#tidl_model_import_ex1">Caffe Trained SSD Based Object Detection Net</a></li>
<li class="level2"><a href="#tidl_model_import_ex2">Caffe Trained Squeeze Net</a></li>
<li class="level2"><a href="#tidl_model_import_ex3">Tensorflow Trained InceptionNet V1</a></li>
<li class="level2"><a href="#tidl_model_import_ex4">ONNX Squeeze Net</a></li>
<li class="level2"><a href="#tidl_model_import_ex5">ONNX with metaArch Import Example</a></li>
<li class="level2"><a href="#tidl_model_import_ex6">tfLite Import Example</a></li>
<li class="level2"><a href="#tidl_model_import_ex7">YUV Import Example</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="tidl_model_import_intro"></a>
Introduction</h1>
<p>TIDL-RT Import (translation) tool can accept a pre-trained floating/fixed point model trained using BVLC-caffe,caffe-jacinto, TensorFlow, TFLite or models exported to ONNX format. Internally it performs various processing and at the end provides model to be inferred on TIDL-RT inference engine.</p>
<p>Below figure shows key processing blocks of this tool and their flow</p>
<div class="image">
<img src="tidl_import_design.png" alt="tidl_import_design.png"/>
<div class="caption">
Functional Blocks of TIDL-RT Importer</div></div>
<p> The tool is available in "ti_dl/utils/tidlModelImport/out" and can be used to import user trained models with below command:</p>
<div class="fragment"><div class="line">Usage:</div><div class="line">&lt; Import tool &gt; &lt; Configuration file &gt; &lt; options &gt;</div><div class="line"></div><div class="line"></div><div class="line">Example:</div><div class="line">./out/tidl_model_import.out ../../test/testvecs/config/import/tidl_import_jacintonet11v2.txt --resizeWidth 512 --resizeHeight 512 </div></div><!-- fragment --><p> Note : Run the Import tool from the "ti_dl/utils/tidlModelImport" directory only. We recommend to use the file path as mentioned in example import configuration file. If you face any issues in file path(s), try with absolute paths.</p>
<p>Note : The trailing options are used to override the parameters in the import configuration file. In above example, the parameters resizeWidth and resizeHeight may have been 128 and 128, respectively in the configuration file (tidl_import_jacintonet11v2.txt), but with override options, import tool will run with the values mentioned in command.</p>
<h1><a class="anchor" id="tidl_model_import_mf"></a>
Supported versions of Model Format</h1>
<p>Proto file from below version are used for validating pre-trained models. In most cases new version models also shall work since the basic operations like convolution, pooling etc don't change. Find more information on migrating to latest version if required in <a class="el" href="md_tidl_fsg_import_tool_design.html">TIDL-RT Importer Design</a></p><ul>
<li>Caffe - 0.17 (caffe-jacinto in gitHub)</li>
<li>Tensorflow - 1.12</li>
<li>ONNX - 1.3.0 (opset 9 and 11)</li>
<li>TFLite - Tensorflow 2.0-Alpha</li>
</ul>
<p><em>Since the Tensorflow 2.0 is planning to drop support for frozen buffer, we recommend to users to migrate to TFlite model format for Tensorflow 1.x.x as well. TFLite model format is supported in both TF 1.x.x and TF 2.x</em><br />
<em>Fixed-point models are only supported for TFLite, and still needs calibration images as input</em></p>
<h1><a class="anchor" id="tidl_model_import_config"></a>
TIDL-RT Import Configuration Parameters</h1>
<p>TIDL-RT importer accepts multiple configuration parameters and all of these can be supplied via a configuration file, see <a href="#tidl_model_import_ex">examples</a> in same page. Most of the parameters are optional and applicable for advanced users. This section explains the purpose of each of this parameter in logically grouped section, however the importer accepts all of these parameters via a single file.</p>
<h2><a class="anchor" id="tidl_model_import_basic"></a>
Basic Configuration Parameters</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">modelType  </td><td class="markdownTableBodyNone">0: Caffe  </td><td class="markdownTableBodyNone">This parameter accepts an integer (0, 1, 2 or 3) indicating which model type is being imported. Following types are currently supported <br />
 0 : Caffe (.caffemodel and .prototxt files)<br />
 1 : TensorFlow (.pb files)<br />
 2 : ONNX (.onnx files)<br />
 3 : TFLite (.tflite files)<br />
   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inputNetFile  </td><td class="markdownTableBodyNone">MUST SET  </td><td class="markdownTableBodyNone">Path to neural network file from training frameworks. Example "deploy.prototxt" from caffe or frozen binary protobuf with parameters from TensorFlow   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inputParamsFile  </td><td class="markdownTableBodyNone">MUST SET if Caffe model  </td><td class="markdownTableBodyNone">Path to neural network parameter file (caffemodel) from BVLC caffe training framework. Not applicable for TensorFlow, TFLite and ONNX   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">outputNetFile  </td><td class="markdownTableBodyNone">MUST SET  </td><td class="markdownTableBodyNone">Path to neural network file produced from TIDL-RT importer having network structure and parameters   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">outputParamsFile  </td><td class="markdownTableBodyNone">MUST SET  </td><td class="markdownTableBodyNone">Path to neural network input and output buffer descriptor file produced from TIDL-RT importer. Refer <a class="el" href="md_tidl_fsg_io_tensors_format.html">TIDL-RT: Input and Output Tensors Format</a> for more details   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">numParamBits  </td><td class="markdownTableBodyNone">8: 8-bit  </td><td class="markdownTableBodyNone">Bit depth for model parameters like kernel coefficients, bias etc. Following values are currently supported<br />
 8 : 8-bit fixed-point<br />
 16 : 16-bit fixed-point<br />
 32 : 32-bit floating-point<br />
 If this value is set to 32 bit then TIDL-RT will run inference in floating point. This feature is only supported in host emulation mode, and one cannot run the models which are imported with numParamBits = 32 on development board.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">numFeatureBits  </td><td class="markdownTableBodyNone">8: 8-bit  </td><td class="markdownTableBodyNone">Bit depth for feature maps of DNN, following values are currently supported<br />
 8 : 8-bit fixed-point<br />
 16 : 16-bit fixed-point   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">quantizationStyle  </td><td class="markdownTableBodyNone">2: TIDL_QuantStyleNP2Fixed  </td><td class="markdownTableBodyNone">Quantization method used by quantization module of TIDL importer. Following values are currently supported<br />
 0: TIDL_QuantStyleFixed<br />
 1: TIDL_QuantStyleDynamic <br />
 2: TIDL_QuantStyleNP2Fixed<br />
 3: TIDL_QuantStyleP2Dynamic<br />
 Refer <a href="itidl__ti_8h.html">TIDL-RT API Guide</a> for more details   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inQuantFactor  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">List to hold scale factor for each input tensor. If the input range used in training is [0:1] and the same is passed as tensor range of [0:255] to TIDL-RT then this parameters shall be 255. This shall not be set in config file when inDataNorm is 1   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inElementType  </td><td class="markdownTableBodyNone">0: 8bit unsigned  </td><td class="markdownTableBodyNone">List to hold element type for each input tensor. Refer <b>TIDL Element Type</b> in <a href="itidl__ti_8h.html">TIDL-RT API Guide</a> for more details. Supported values are 0 : 8bit Unsigned, 1 : 8bit Signed, 2 : 16bit Unsigned, 3 : 16bit Signed, 6 : Single Precision Float   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inWidth  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List to hold width of each input tensor. If set, this value will overwrite the width from the neural net. If not set, the importer will use the original size in neural network.<br />
 <b>Note</b>:It is expected that neural net can support different resolution other than what its trained for reliable behavior   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inHeight  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List to hold height of each input tensor. If set, this value will overwrite the height from the neural net. If not set, the importer will use the original size in neural network.<br />
 <b>Note</b>:It is expected that neural net can support different resolution other than what its trained for reliable behavior   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inNumChannels  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List to hold "number of channels" of each input tensor. If set, this value will overwrite the number of channels from the neural net. If not set, the importer will use the original size in neural network. <b>Note</b>: It is expected that neural net can support different resolution other than what its trained for reliable behavior   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">numBatches  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">Number of batches to be processed together, it can take values &gt;= 1. By using this parameter, one can improve the throughput at cost of higher latency.<br />
 <b>Note</b>: Please use this parameter carefully as it is not always necessary to have higher throughput with higher number of batches   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inDataNamesList  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List of input tensor names in network to be imported(use space/comma/tab to split). It is useful to import portion of network. If not set, importer will search the input net file for default inputs.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">outDataNamesList  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List of output tensor names in network to be imported(use space/comma/tab to split). It is useful to import portion of network. If not set, importer will search the input net file for default outputs.   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_ip"></a>
Configuration Parameters for Input Pre-Processing</h2>
<p>In most cases, the input to the network gets processed using some image transform function. For example, mean subtraction, scaling or standard deviation, cropping etc. The parameters mentioned below, gets embedded in TIDL-RT's model file and are used during inference.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inDataNorm  </td><td class="markdownTableBodyNone">0: Disable  </td><td class="markdownTableBodyNone">Enable / Disable Normalization on input tensor. Scale and mean values are applicable only if this is enabled. Please refer <a href="md_tidl_fsg_steps_to_debug_mismatch.html#did_tidl_debug_fimage_norm_2">Input Normalization</a> page to understand its usage.<br />
 <b>Note:</b> This parameter adds a batchnorm layer to the TIDL network which can potentially be merged to following convolution layer if foldPreBnConv2D = 1. If this batchnorm layer is merged to convolution layer then this operation happens at no impact on processing time   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inMean  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List: Mean value needs to be subtracted for each channel of all input tensors. Only applicable when inDataNorm = 1. Please refer <a href="md_tidl_fsg_steps_to_debug_mismatch.html#did_tidl_debug_fimage_norm_2">Input Normalization</a> page to understand its usage   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inScale  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">List: Scale value needs to be multiplied after mean subtraction for each channel of all input tensors. Only applicable when inDataNorm = 1. Please refer <a href="md_tidl_fsg_steps_to_debug_mismatch.html#did_tidl_debug_fimage_norm_2">Input Normalization</a> page to understand its usage   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inDataPadInTIDL  </td><td class="markdownTableBodyNone">0: Disable  </td><td class="markdownTableBodyNone">This parameter controls enable/disable of padding to be performed on input tensor in TIDL-RT inference.<br />
 On enable, pad layer gets inserted in beginning of neural net if required.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inDataFormat  </td><td class="markdownTableBodyNone">1: RGB planar  </td><td class="markdownTableBodyNone">List: Input tensor color format. 0: BGR planar, 1: RGB planar. Refer <a href="itidl__ti_8h.html#ab6b0fc2c95abf8a22eff9b7413bcc796">TIDL API guide</a> for supported list of formats   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inResizeType  </td><td class="markdownTableBodyNone">0: Normal  </td><td class="markdownTableBodyNone">Image resize type . 0: normal resize, 1: keep aspect ratio. Refer <a href="itidl__ti_8h.html#a6a57046fc81eacab7929f999cdc81934">TIDL API guide</a> for supported formats list.<br />
 <b>Note:</b> This resize is not part of TIDL-RT inference, instead its done in the test bench code. User is expected to take care of this resize in their final application before giving the input to TIDL-RT.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">resizeWidth  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">If resizeWidth is set and resizeWidth &gt;= inWidth, then first input image is resized to resizeWidth and center cropping of inWidth is done on this resized image to get data for inference.<br />
 resizeWidth &lt; inWidth is not a valid configuration.<br />
 <b>Note:</b> This resize is not part of TIDL-RT inference, instead its done in the test bench code. User is expected to take care of this resize in their final application before giving the input to TIDL-RT.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">resizeHeight  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">If resizeHeight is set and resizeHeight &gt;= inHeight, then first input image is resized to resizeHeight and center cropping of inHeight is done on this resized image to get data for inference.<br />
 resizeHeight &lt; inHeight is not a valid configuration.<br />
 <b>Note:</b> This resize is not part of TIDL-RT inference, instead its done in the test bench code. User is expected to take care of this resize in their final application before giving the input to TIDL-RT.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">outputFeature16bitNamesList  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">This parameter is useful for mixed-precision inference of neural net.<br />
 It is a list of names of the output layers (separated by comma or space or tab) as in the original model whose feature/activation output user wants to be in 16 bit.<br />
 <b>Note</b>: If for a given layer feature/activations is in 16 bit then parameters will automatically become 16 bit and user need not specify them as part of params16bitNamesList.<br />
 Since TIDL-RT importer merges certain layers, this list should correspond to the last layer after merging. User can find this mapping by executing the importer once without this parameter which generates &lt;outputNetFile&gt;.layer_info.txt where outputNetFile is same as given by the user in the import config file. This *.layer_info.txt file contains three columns the first one is the layer number, second one is unique data id and last one is the name as given in the original network's model format. This third column gives the name which will be present in the TIDL imported network after merging any layer. User should use this value for outputFeature16bitNamesList.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">params16bitNamesList  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">This parameter is useful for mixed-precision inference of neural net.<br />
 It is a list of names of the output layers (separated by comma or space or tab) as in the original model whose parameters user wants to be in 16 bit. This is not the name of the parameter of the layer but is expected to be the output name of the layer.<br />
 <b>Note</b>:If a given layers feature/activations is in 16 bit then parameters will automatically become 16 bit even if its not part of this list.<br />
 Since TIDL-RT importer merges certain layers, this list should correspond to the last layer in the merged output. User can find this mapping by running the import tool once without this parameter which generates &lt;outputNetFile&gt;.layer_info.txt where outputNetFile is same as given by the user in the import config file. This *.layer_info.txt file contains three columns the first one is the layer number, second one is unique data id and last one is the name as given in the original network's model format. This third column gives the name which will be present in the TIDL imported network after merging any layer. User should use this value for params16bitNamesList.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inYuvFormat  </td><td class="markdownTableBodyNone">-1: Not used  </td><td class="markdownTableBodyNone">This parameter is useful when network is trained with RGB input but the input available during inference is in YUV format. Following values are currently supported<br />
 -1: No format conversion is performed and the input is directly used for neural net inference<br />
 0 (TIDL_inYuvFormatYuv420_NV12): YUV to RGB conversion performed considering input in YUV 4:2:0 NV12 format   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">foldPreBnConv2D  </td><td class="markdownTableBodyNone">1: Enable  </td><td class="markdownTableBodyNone">This parameter controls folding batch norm layer with following convolution layer. Following values are currently supported<br />
 0: Disable<br />
 1: Enable, this will improve run time performance but may result in slight accuracy drop   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">metaArchType  </td><td class="markdownTableBodyNone">-1: Not used  </td><td class="markdownTableBodyNone">This parameter is applicable for neural networks of object detection task and is used to inform the underneath architecture used for post processing of object detection. Please refer <a href="itidl__ti_8h.html#acc40b635fa4fa49a8b5923697050599f.html">TIDL-RT API Guide</a> for supported meta architectures. If metaArch is not used, please leave it default. Please refer <a class="el" href="md_tidl_fsg_meta_arch_support.html">Meta Architectures Support</a> for more details.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">metaLayersNamesList  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Configuration files describing the details of meta architecture, example pipeline.config for TF SSD. Please refer <a class="el" href="md_tidl_fsg_meta_arch_support.html">Meta Architectures Support</a> for more details.   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_path"></a>
Configuration Parameters for path of different modules</h2>
<p>As explained earlier, importer is front end utility for users to perform all processing which happens during compile stage. Internal to importer, it makes call to other modules which are independent executables. Parser, optimizer and quantizer are part of the importer whereas graph compiler, graph visualizer are independent modules being used by importer. Quantizer module of the importer also makes use of TIDL-RT inference engine for statistics collection and hence it needs path of it.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">tidlStatsTool  </td><td class="markdownTableBodyNone">../../test/PC_dsp_test_dl_algo.out(.exe)  </td><td class="markdownTableBodyNone">Host emulation executable file path of TIDL-RT inference. It is used by quantizer module for range collection. If not specified, uses from default location   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">perfSimTool  </td><td class="markdownTableBodyNone">../../utils/perfsim/ti_cnnperfsim.out(.exe)  </td><td class="markdownTableBodyNone">Graph Compiler executable file path. If not specified, uses from default location   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">graphVizTool  </td><td class="markdownTableBodyNone">../../utils/tidlModelGraphviz/out/tidl_graphVisualiser.out(.exe)  </td><td class="markdownTableBodyNone">Path of the tool to generate graphical visual output . If not specified, uses from default location   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">modelDumpTool  </td><td class="markdownTableBodyNone">../../utils/tidlModelDump/out/tidl_dump.out(.exe)  </td><td class="markdownTableBodyNone">Similar to graphViz tool which provides the complete information about neural net after complete operation of importer. Only difference is that it is textual information instead of visual. It is optional parameter   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_infer_quantization"></a>
Configuration Parameters related to TIDL-RT inference for quantization</h2>
<p>As explained in previous section, quantization module in importer makes use of host emulation mode of TIDL-RT. So importer also accepts the parameters related to TIDL-RT inference and prepares the configuration file for TIDL-RT host emulation inference. These parameters are described in this section</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inData  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inFileFormat  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">numFrames  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">postProcType  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">postProcDataId  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">quantRangeUpdateFactor  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">debugTraceLevel  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">writeTraceLevel  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">rawDataInElementType  </td><td class="markdownTableBodyNone">0: 8bit unsigned  </td><td class="markdownTableBodyNone">Please refer the <a href="md_tidl_sample_test.html#tidl_inference_2">TIDL Sample Application doc</a>   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_quantization"></a>
Configuration Parameters for quantization</h2>
<p>Quantizer is one of the critical module in importer and a dedicated <a class="el" href="md_tidl_fsg_quantization.html">page</a> is provided for better explanation of it. Users are recommended to read the page to get a better idea of different control parameters related to quantization</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">calibrationOption  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">Refer <a class="el" href="md_tidl_fsg_quantization.html">Quantizer</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">activationRangeMethod  </td><td class="markdownTableBodyNone">0 : TIDL_ActivationRangeMethodHistogram  </td><td class="markdownTableBodyNone">[Optional parameter]<br />
 Various Activation range methods supported by TIDL. The following types are currently supported<br />
 0 : TIDL_ActivationRangeMethodHistogram<br />
 This parameter accepts an integer indicating which activation range method to be used.<br />
 This option is only applicable if calibrationOption is set to TIDL_CalibOptionActivationRange.<br />
 The default value is "0" (Fixed)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">weightRangeMethod  </td><td class="markdownTableBodyNone">1 : TIDL_WeightRangeMethodMedian  </td><td class="markdownTableBodyNone">Various Weight range methods supported by TIDL. The following types are currently supported<br />
 0 : TIDL_WeightRangeMethodHistogram<br />
 1 : TIDL_WeightRangeMethodMedian<br />
 This parameter accepts an integer indicating which weight range method to be used.<br />
 This option is only applicable if calibrationOption is set to TIDL_CalibOptionWeightRange.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">percentileActRangeShrink  </td><td class="markdownTableBodyNone">0.01  </td><td class="markdownTableBodyNone">Refer <a class="el" href="md_tidl_fsg_quantization.html">Quantizer</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">percentileWtRangeShrink  </td><td class="markdownTableBodyNone">0.01  </td><td class="markdownTableBodyNone">[Optional parameter]<br />
 This parameter is only applicable when weightRangeMethod is TIDL_WeightRangeMethodHistogram. This is percentile of the total number of elements in a weight filter which needs to be discarded from both side of weight distribution. If input is unsigned then this is applied to only one side of weight distribution.<br />
 For example percentileRangeShrink = 0.01, means to discard 1/10000 elements from both or one side of weight distribution.   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">biasCalibrationFactor  </td><td class="markdownTableBodyNone">0.05  </td><td class="markdownTableBodyNone">Refer <a class="el" href="md_tidl_fsg_quantization.html">Quantizer</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">biasCalibrationIterations  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Refer <a class="el" href="md_tidl_fsg_quantization.html">Quantizer</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">numFramesBiasCalibration  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">Refer <a class="el" href="md_tidl_fsg_quantization.html">Quantizer</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">quantRangeExpansionFactor  </td><td class="markdownTableBodyNone">1.0  </td><td class="markdownTableBodyNone">Margin that needs to be applied on feature map Range.<br />
 Example 1.2 would apply 20% margin to range values   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_gc"></a>
Configuration Parameters for Graph Compiler</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">executeNetworkCompiler  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">Enable(1)/Disable(0) execution of graph compiler, graph compiler can only be disabled for host emulation reference flow to speed up execution but it is mandatory to keep this flag enabled for execution on hardware   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">compileConstraintsFlag  </td><td class="markdownTableBodyNone"></td><td class="markdownTableBodyNone">This is a control flag used to control internal behavior of graph compiler and created to apply workarounds in case user faces any issue. It is not expected for users to change it without guidance from TI   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">perfSimConfig  </td><td class="markdownTableBodyNone">../../test/testvecs/config/import/device_base.cfg  </td><td class="markdownTableBodyNone">Graph Compiler configuration file to provide device specific information, this file has description of the additional control parameters which are not described here and not accepted by importer   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">msmcSize  </td><td class="markdownTableBodyNone">-1  </td><td class="markdownTableBodyNone">Size of the L3 (MSMC) memory in KB which can be used by TIDL-RT: -1 (default), in default case the size will be picked from the size given in perfSimConfig file. Any other value overrides the value mentioned in perfSimConfig file   </td></tr>
</table>
<h2><a class="anchor" id="tidl_model_import_fc"></a>
Configuration Parameters for format conversion</h2>
<p>TIDL-RT natively supports only NCHW layout and fixed point inference, but in order to allow integration with other runtimes, it allows accepting input tensor in floating point and as well in NWHC layout. Similarly on output side, TIDL-RT can produce floating point tensor and NWHC layout. This is achieved by inserting a format conversion layer at appropriate end. Below table describes the configuration parameters </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Parameter  </th><th class="markdownTableHeadNone">Default  </th><th class="markdownTableHeadNone">Description   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">addDataConvertToNet  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">This is a bit field parameter, bit 0 would add for data convert layer at input and bit 1 would add in output   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inZeroPoint  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">List to indicate zero point of each fixed point input tensor   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">inLayout  </td><td class="markdownTableBodyNone">0: TIDL_LT_NCHW  </td><td class="markdownTableBodyNone">List to indicate data Layout of each input tensor. Following types are currently supported:<br />
 0 : TIDL_LT_NCHW<br />
 1 : TIDL_LT_NHWC   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">inTensorScale  </td><td class="markdownTableBodyNone">1.0  </td><td class="markdownTableBodyNone">List to indicate scale of each input tensor in float, This is alias for inQuantFactor   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">outElementType  </td><td class="markdownTableBodyNone">0: 8bit unsigned  </td><td class="markdownTableBodyNone">List to indicate element type for each output tensor. Refer <b>TIDL Element Type</b> in <a href="itidl__ti_8h.html">TIDL-RT API Guide</a> for more details. Supported values are 0 : 8bit Unsigned, 1 : 8bit Signed, 2 : 16bit Unsigned, 3 : 16bit Signed, 6 : Single Precision Float   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">outZeroPoint  </td><td class="markdownTableBodyNone">0  </td><td class="markdownTableBodyNone">List to indicate zero point of each fixed point output tensor   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">outLayout  </td><td class="markdownTableBodyNone">0: TIDL_LT_NCHW  </td><td class="markdownTableBodyNone">List to indicate data Layout of each output tensor. Following types are currently supported:<br />
 0 : TIDL_LT_NCHW<br />
 1 : TIDL_LT_NHWC   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">outTensorScale  </td><td class="markdownTableBodyNone">1.0  </td><td class="markdownTableBodyNone">List to indicate scale of each output tensor in float.   </td></tr>
</table>
<h1><a class="anchor" id="tidl_model_import_ex"></a>
Examples</h1>
<p>Configuration files used for validation of models are provided in the "ti_dl/test/testvecs/config/import/" folder for reference. A few example configuration files are given below:</p>
<h2><a class="anchor" id="tidl_model_import_ex1"></a>
Caffe Trained SSD Based Object Detection Net</h2>
<ul>
<li>[modelType] This is a caffe model(prototxt + caffemodel).</li>
<li>[inDataFormat] Input data format is BGR planar. The input size will get from original prototxt(no overwrite). By default, import tool will load image file using OpenCV and resize it and convert it to BGR planar as input.</li>
<li>[inData] Images for range collection are listed in "detection_list.txt" text file. By default, the list contains JPEG/PNG compressed file list.</li>
<li>[postProcType] The result is bounding box list, post process is to draw bounding box on original image.</li>
</ul>
<div class="fragment"><div class="line">modelType          = 0</div><div class="line">inputNetFile       = &quot;../../test/testvecs/models/public/caffe/jdetNet512x512/deploy.prototxt&quot;</div><div class="line">inputParamsFile    = &quot;../../test/testvecs/models/public/caffe/jdetNet512x512/voc0712_ssdJacintoNetV2_iter_106000.caffemodel&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/caffe/tidl_net_jdetNet_512x512.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/caffe/tidl_io_jdetNet_512x512_&quot;</div><div class="line">inDataFormat = 0</div><div class="line">inData  =   &quot;../../test/testvecs/config/detection_list.txt&quot;</div><div class="line">postProcType = 2</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex2"></a>
Caffe Trained Squeeze Net</h2>
<ul>
<li>[modelType] This is a caffe model(prototxt + caffemodel).</li>
<li>[inDataNorm] Input data need to be normalized, since the model is trained based on a specified range.</li>
<li>[inMean &amp; inScale] The import tool should use following value for input data normalization.</li>
<li>[inDataFormat] Input data format is BGR planar. The input size will be set to resize width &amp; height. By default, import tool will load image file using OpenCV, resize it and convert it to BGR planar as input.</li>
<li>[inData] Images for range collection are listed in "detection_list.txt" text file. By default, the list contains JPEG/PNG compressed file list.</li>
<li>[postProcType] The result is classification. The post process is to tell me the top-1 class name(by default, ImageNet is used).</li>
</ul>
<div class="fragment"><div class="line">modelType          = 0</div><div class="line">inputNetFile       = &quot;../../test/testvecs/models/public/caffe/squeezeNet1.1/deploy.prototxt&quot;</div><div class="line">inputParamsFile    = &quot;../../test/testvecs/models/public/caffe/squeezeNet1.1/squeezenet_v1.1.caffemodel&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/caffe/tidl_net_suqeezenet_1_1.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/caffe/tidl_io_suqeezenet_1_1_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 104 117 123</div><div class="line">inScale = 1 1 1</div><div class="line">inDataFormat = 0</div><div class="line">resizeWidth = 256</div><div class="line">resizeHeight = 256</div><div class="line">inData = ../../test/testvecs/config/imageNet_sample_val.txt</div><div class="line">postProcType = 1</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex3"></a>
Tensorflow Trained InceptionNet V1</h2>
<ul>
<li>[modelType] This is a tensorflow pb file.</li>
<li>[inDataNorm] Input data need to be normalized, since the model is trained based on specified range.</li>
<li>[inMean &amp; inScale] The import tool should use following value for input data normalization.</li>
<li>[resize] By default, input data format is RGB planar. The input size will be set to inWidth &amp; inHeight. By default, import tool will load image file using OpenCV, resize it, crop it and convert it to RGB planar as input.</li>
<li>[inData] Images for range collection are listed in "detection_list.txt" text file. By default, the list contains JPEG/PNG compressed file list.</li>
<li>[postProcType] The result is classification. The post process is to tell me the top-1 class name(by default, ImageNet is used).</li>
</ul>
<div class="fragment"><div class="line">modelType          = 1</div><div class="line">inputNetFile      = &quot;../../test/testvecs/models/public/tensorflow/inceptionNetv1/inception_v1_fbn.pb&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/tensorflow/tidl_net_inception_v1.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/tensorflow/tidl_io_inception_v1_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 128 128 128</div><div class="line">inScale =  0.0078125 0.0078125 0.0078125</div><div class="line">resizeWidth = 256</div><div class="line">resizeHeight = 256</div><div class="line">inWidth  = 224</div><div class="line">inHeight = 224 </div><div class="line">inNumChannels = 3</div><div class="line">inData = ../../test/testvecs/config/imageNet_sample_val_bg.txt</div><div class="line">postProcType = 1</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex4"></a>
ONNX Squeeze Net</h2>
<ul>
<li>[modelType] This is a ONNX file.</li>
<li>[inDataNorm] Input data need to be normalized, since the model is trained based on specified range.</li>
<li>[inMean &amp; inScale] The import tool should use following value for input data normalization.</li>
<li>[resize] By default, input data format is RGB planar. The input size will be set to inWidth &amp; inHeight. By default, import tool will load image file using OpenCV, resize it, crop it and convert it to RGB planar as input.</li>
<li>[inData] Images for range collection are listed in "detection_list.txt" text file. By default, the list contains JPEG/PNG compressed file list.</li>
<li>[postProcType] The result is classification. The post process is to tell me the top-1 class name(by default, ImageNet is used).</li>
</ul>
<div class="fragment"><div class="line">modelType          = 2</div><div class="line">inputNetFile      = &quot;../../test/testvecs/models/public/onnx/squeezenet1.1.onnx&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/onnx/tidl_net_squeezenet1.1.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/onnx/tidl_io_squeezenet1.1_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 123.675 116.28 103.53</div><div class="line">inScale = 0.017125 0.017507 0.017429</div><div class="line">resizeWidth = 256</div><div class="line">resizeHeight = 256</div><div class="line">inWidth  = 224</div><div class="line">inHeight = 224 </div><div class="line">inNumChannels = 3</div><div class="line">inData = ../../test/testvecs/config/imageNet_sample_val.txt</div><div class="line">postProcType = 1</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex5"></a>
ONNX with metaArch Import Example</h2>
<ul>
<li>[metaLayersNamesList] add SSD metaArch to the backbone.</li>
</ul>
<div class="fragment"><div class="line">modelType          = 2</div><div class="line">numParamBits       = 8</div><div class="line">numFeatureBits     = 8</div><div class="line">quantizationStyle  = 2</div><div class="line">inputNetFile       = &quot;../../test/testvecs/models/internal/onnx/tiad_ssd/softmax/ssd_lite_mobilenet_v1_tiod_epoch_6.onnx&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/onnx/tidl_net_tiad_ssd.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/onnx/tidl_io_tiad_ssd_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 104 117 123</div><div class="line">inScale = 1 1 1</div><div class="line">inDataFormat = 1</div><div class="line">inWidth  = 768</div><div class="line">inHeight = 384 </div><div class="line">inNumChannels = 3</div><div class="line">metaArchType = 3</div><div class="line">numFrames = 3</div><div class="line">metaLayersNamesList = ../../test/testvecs/models/internal/onnx/tiad_ssd/softmax/tidl_meta_arch.prototxt</div><div class="line">inData  =   &quot;../../test/testvecs/config/tiad_bmp_list.txt&quot;</div><div class="line">postProcType = 2</div><div class="line">perfSimConfig = ../../test/testvecs/config/import/perfsim_base.cfg</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex6"></a>
tfLite Import Example</h2>
<div class="fragment"><div class="line">modelType          = 3</div><div class="line">numParamBits      = 15</div><div class="line">quantizationStyle  = 2</div><div class="line">inputNetFile      = ../../test/testvecs/models/public/tflite/mobilenet_v1_1.0_224.tflite</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/tflite/tidl_net_tflite_mobilenet_v1_1.0_224.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/tflite/tidl_io_tflite_mobilenet_v1_1.0_224_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 128 128 128</div><div class="line">inScale =  0.0078125 0.0078125 0.0078125</div><div class="line">resizeWidth = 256</div><div class="line">resizeHeight = 256</div><div class="line">inWidth  = 224</div><div class="line">inHeight = 224 </div><div class="line">inNumChannels = 3</div><div class="line">inData = ../../test/testvecs/config/imageNet_sample_val_bg.txt</div><div class="line">postProcType = 1</div></div><!-- fragment --><h2><a class="anchor" id="tidl_model_import_ex7"></a>
YUV Import Example</h2>
<div class="fragment"><div class="line">modelType          = 1</div><div class="line">numParamBits      = 8</div><div class="line">quantizationStyle  = 2</div><div class="line">inputNetFile      = &quot;../../test/testvecs/models/public/tensorflow/mobileNet_v2/mobilenet_v2_1.0_224_final.pb&quot;</div><div class="line">outputNetFile      = &quot;../../test/testvecs/config/tidl_models/tensorflow/tidl_net_mobilenet_v2_1.0_224_yuv.bin&quot;</div><div class="line">outputParamsFile   = &quot;../../test/testvecs/config/tidl_models/tensorflow/tidl_net_mobilenet_v2_1.0_224_yuv_&quot;</div><div class="line">inDataNorm  = 1</div><div class="line">inMean = 128 128 128</div><div class="line">inScale =  0.0078125 0.0078125 0.0078125</div><div class="line">inWidth  = 224</div><div class="line">inHeight = 224</div><div class="line">inNumChannels = 1</div><div class="line">inData  =   &quot;../../test/testvecs/input/airshow_yuv420_224x224_224x112.yuv&quot;</div><div class="line">perfSimConfig = &quot;../../test/testvecs/config/import/device_config.cfg&quot;</div><div class="line">postProcType = 1</div><div class="line">inFileFormat = 1</div><div class="line">rawDataInElementType = 0</div><div class="line">foldPreBnConv2D = 1</div><div class="line">inYuvFormat = 0</div><div class="line">numFrames   = 1</div><div class="line">compileConstraintsFlag = 579</div></div><!-- fragment --> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
