<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>TI Deep Learning Product User Guide: TIDL-RT: Troubleshooting Guide for Accuracy/Functional Issues</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="ti_logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TI Deep Learning Product User Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_tidl_fsg_steps_to_debug_mismatch.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">TIDL-RT: Troubleshooting Guide for Accuracy/Functional Issues </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#did_tidl_debug_error_intro">Steps to Debug Error Scenarios for target(EVM) execution</a></li>
<li class="level1"><a href="#did_tidl_debug_func_intro">Steps to Debug Functional Mismatch in Host emulation</a></li>
<li class="level1"><a href="#did_tidl_debug_pre_processing_mismtach">Steps to narrow down pre-processing mismatches</a><ul><li class="level2"><a href="#did_tidl_debug_fimage_norm_1">1. Input Color Format</a></li>
<li class="level2"><a href="#did_tidl_debug_fimage_norm_2">2. Image Normalization</a></li>
</ul>
</li>
<li class="level1"><a href="#did_tidl_debug_fimage_norm_3">Comparing the input Tensor to TIDL-RT with Reference</a></li>
<li class="level1"><a href="#did_tidl_debug_wgt_quant">Weights Quantization statistic Analysis</a></li>
<li class="level1"><a href="#did_tidl_debug_Fm_scale_quant">Feature Map Scale Analysis</a></li>
<li class="level1"><a href="#did_tidl_debug_fm_quant">Feature Map Comparison with Reference</a><ul><li class="level2"><a href="#did_tidl_debug_caffe_infer">Example script to Reference output from Caffe</a></li>
<li class="level2"><a href="#did_tidl_debug_onnx_mxnet_infer">Example script to Reference output from MxNet for ONNX</a></li>
<li class="level2"><a href="#did_tidl_debug_tf_infer">Example script to Reference output from TensorFlow for Frozen Graph</a></li>
<li class="level2"><a href="#did_tidl_debug_tflite_infer">Example script to Reference output from TensorFlow for TFLite model</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="did_tidl_debug_error_intro"></a>
Steps to Debug Error Scenarios for target(EVM) execution</h1>
<ul>
<li>It is recommended to first test the inference of a given network in host emulation mode to make sure output is as per expectation.</li>
<li>If execution on target(EVM) is not completing then user can enable more verbose logs by setting debugTraceLevel = 2 in the TIDL-RT inference configuration file. This option enables more detailed traces which will help in identifying the layer which is behaving unexpectedly.</li>
<li>If the user is finding target(EVM) execution output for a given network to be different from the host emulation output for the same, then they should follow the following steps to identify which layer causes the first mismatch:<ul>
<li>Enable layer level fixed point traces in both host emulation and EVM execution. This can be done by setting writeTraceLevel = 1 in the TIDL-RT inference configuration file.</li>
<li>Run the TIDL-RT inference on host emulation mode. Layer level traces will be generated inside ti_dl/test/traces folder. If this folder is missing, please create the same. Keep a copy of this trace folder, lets call it traces_ref</li>
<li>Repeat the above step for target(EVM) execution. Keep a copy of this trace folder, lets call it traces_target</li>
<li>Use any binary comparison tool (e.g. beyond compare, diff, cmp etc) to compare the traces_ref and traces_target folder</li>
<li>The outputs are expected to bit match and if any particular layer output is mismatching in these two modes then the first layer where mismatch is observed is the layer which needs to be reported.</li>
</ul>
</li>
<li>debugTraceLevel and writeTraceLevel can also be enabled from SDK application. For these option to work on target(EVM) device it is important to follow all the steps as mentioned <a href="md_tidl_sample_test.html#tidl_inference_4">here</a></li>
</ul>
<h1><a class="anchor" id="did_tidl_debug_func_intro"></a>
Steps to Debug Functional Mismatch in Host emulation</h1>
<ul>
<li>The following steps are suggested, when user find the functional issue (Lower accuracy) while running inference using TIDL-RT compared to Floating model inference on Training framework (Caffe, tensorflow, Pytorch etc). All these steps need to be performed in host emulation mode rather than on target(EVM) device. Once user achieves the acceptable accuracy then, host emulation and target execution on device can be bit-matched. <br />
 - First step is to run the model in floating point mode and see if accuracy is same as what you get in your training framework. To run the model in float you will have to set numParamBits = 32 in TIDL-RT import config file and run import and inference again. (<b>Note that this flow is not optimized for our SOC's and is supported only in host emulation mode and itâ€™s meant only for debug purpose</b>).<ul>
<li>If this accuracy matches then this indicates that there is no gap in pre-processing done on the input in your training framework and TIDL-RT inference.</li>
<li>The next step would be to try advanced calibration options to improve the accuracy. We recommend that you try your model with calibrationOption = 7 with sufficient number of images used for calibration (Typically any number higher than 50 gives good results). The details of this option can be found in <a href="md_tidl_fsg_quantization.html#did_tidl_quantization_1">here</a>, refer section on Advanced Bias Calibration.</li>
<li>Even after advanced calibration if the result 8-bit quantization results are not acceptable then 8-bit quantization is not sufficient for your network. We recommend to try 16-bit quantization for your network (Note that this option will results into more compute time for inference).</li>
<li>If 16 bit results are as per expectation then user can try mixed precision to manually increase the precision of activations/parameters of certain layers. The details of this option can be found in <a href="md_tidl_fsg_quantization.html#did_tidl_quantization_1">here</a>, refer section on Mixed Precision. To choose which layers user will have to analyze the activation/parameters statistics. User can refer <a href="#did_tidl_debug_fm_quant">here</a> to view sample script given to generate such statistic or if required they can extend it to generate more statistics.</li>
<li>The accuracy is not acceptable with floating point inference also then, this typically indicates that pre-processing applied in your training framework and the one applied in TIDL are not the same. Follow the steps mentioned in <a href="#did_tidl_debug_pre_processing_mismtach">Steps to narrow down pre-processing mismatches</a> section to narrow down the issues related to pre-processing.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="did_tidl_debug_pre_processing_mismtach"></a>
Steps to narrow down pre-processing mismatches</h1>
<ul>
<li>Follow the following steps to narrow down the mismatches because of differences in pre-processing in TIDL-RT compared to the pre-processing applied in the training framework. </li>
</ul>
<h2><a class="anchor" id="did_tidl_debug_fimage_norm_1"></a>
1. Input Color Format</h2>
<ul>
<li>If you are using inFileFormat = 0 or 2 (Compressed images) and the decoded image could be using the color format as RGB or BGR in the training code. This color format shall be properly informed to TIDL-RT import step by setting inDataFormat.</li>
<li>If the user using openCV in the training code, the decoded image default would be BGR format.</li>
<li>If the user using PIL (Pillow) in the training code, the decoded image default would is RGB format.</li>
<li>Most of the models trained in Pytorch and Tensorflow are in RGB format. <b>Note:</b> This may not be always true.</li>
<li>Most of the models trained Caffe are in BGR format. <b>Note:</b> This may not be always true.</li>
<li>You can set this format by appropriately setting <a href="md_tidl_model_import.html#tidl_model_import_3">inDataFormat</a> variable in TIDL-RT import config file. Set the value to be 0 if you are using BGR planar format or set it to 1 if you are using RGB planar format.</li>
</ul>
<h2><a class="anchor" id="did_tidl_debug_fimage_norm_2"></a>
2. Image Normalization</h2>
<ul>
<li>In the training code, image normalization is the most common practice.</li>
<li>Various normalization methods are used by user in their training code based on the training framework.</li>
<li>These normalization information can be configured in TIDL-RT through importer step.</li>
<li>Set <a href="md_tidl_model_import.html#tidl_model_import_3">inDataNorm</a> = 1 &amp;&amp; corresponding inMean and inScale parameters to enable this</li>
<li>In TIDL inMean and inScale values are uses as follows on the input RGB/ BGR tensor having a range 0 - 255 for each channel.<ul>
<li>image = image - inMean</li>
<li>image = image * inScale</li>
</ul>
</li>
<li>The above computation is implemented in TIDL-RT by adding a BatchNorm Layer before passing input tensor to first processing layer.</li>
<li>The advantage of doing this within TIDL-RT inference engine is that typically this BatchNorm layer can get merged to following convolution layers and hence the pre-processing can come at no extra run time performance cost. Note that for this merge to happen foldPreBnConv2D should be set to 1 ( default value is 1).</li>
<li>For example in Caffe, if a user has below transformation parameter, then inMean and inScale shall be programmed as below <div class="fragment"><div class="line">transform_param {</div><div class="line">  scale: 0.017</div><div class="line">  mean_value: [103.94, 116.78, 123.68]</div><div class="line">}</div></div><!-- fragment --></li>
<li>inMean and inScale shall be programmed as below in import configuration file for the above transform <div class="fragment"><div class="line">inDataNorm  = 1</div><div class="line">inMean = 103.94, 116.78, 123.68</div><div class="line">inScale = 0.017 0.017 0.017</div></div><!-- fragment --></li>
<li>In PyTorch, if the user is using below transformation on image with range of 0 to 1 <div class="fragment"><div class="line">transform = transforms.Compose([</div><div class="line">    transforms.ToTensor(),</div><div class="line">    transforms.Normalize([0.485, 0.456, 0.406], # mean</div><div class="line">    [0.229, 0.224, 0.225]) # standard_deviation</div><div class="line">])</div></div><!-- fragment --></li>
<li>The above is equivalent of below <div class="fragment"><div class="line">image = image(in Float32) / 255.0</div><div class="line">image = image - mean</div><div class="line">image = image / standard_deviation</div></div><!-- fragment --></li>
<li>So the inMean and inScale shall be programmed as below in import configuration file for the above transform <div class="fragment"><div class="line">inMean = (mean*255) (In float)</div><div class="line">inScale  = 1/(255*standard_deviation) (In float)</div><div class="line"></div><div class="line">inMean = 123.675 116.28 103.53</div><div class="line">inScale = 0.017125 0.017507 0.017429</div></div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="did_tidl_debug_fimage_norm_3"></a>
Comparing the input Tensor to TIDL-RT with Reference</h1>
<ul>
<li>It is important to match the input tensor to TIDL net with the input tensor of network which was trained.</li>
<li>Save the input tensor from the training code that you are using in float format.</li>
<li>Use writeTraceLevel = 3 in TIDL-RT import config file to write the layer level floating point traces from the TIDL-RT to files.</li>
<li>By default the data normalizing batchNorm layer is merged to following convolution layer. So set foldPreBnConv2D = 0 in TIDL-RT import config file to avoid this.</li>
<li>Compare the output of this batchNorm layer with input tensor from training code. Refer <a href="#did_tidl_debug_fm_quant">Link</a></li>
</ul>
<h1><a class="anchor" id="did_tidl_debug_wgt_quant"></a>
Weights Quantization statistic Analysis</h1>
<ul>
<li>The TIDL-RT import tool generates parameter quantization statistics at end of model import. This information is saved as "*_paramDebug.csv" file in the same location as output TIDL model files. Following is a sample example of information generated in this file ( Note: This information is calculated using the floating point weights and quantized weights in function "TIDL_CompareParams" (File : ti_dl/utils/tidlModelImport/tidl_import_quantize.cpp). User can refer to this code for more information on each parameter).</li>
</ul>
<div class="image">
<img src="Weights_Quantization_statistic.png" alt="Weights_Quantization_statistic.png"/>
<div class="caption">
Weights Quantization statistic</div></div>
<p> The important information that needs to be check during debugging is mean and max of all the absolute float parameters "meanOrigFloat" and "orgmax". If the orgmax is much higher than the meanOrigFloat then there is possibility of higher quantization loss. Refer (!tidl_fsg_quantization.md::did_tidl_quantization_Types) to reduce the quantization loss in inference. User can compare this file with 16-bit and 8-bit parameters.</p>
<h1><a class="anchor" id="did_tidl_debug_Fm_scale_quant"></a>
Feature Map Scale Analysis</h1>
<ul>
<li>If user could not find any information regarding the quantization error, the next steps is to analyze the feature/activation maps scales used during the quantization for the same.</li>
<li>This can be achieved by enabling layer level traces from TIDL-RT inference by setting below</li>
</ul>
<div class="fragment"><div class="line">debugTraceLevel = 1</div><div class="line">writeTraceLevel = 3</div></div><!-- fragment --><ul>
<li>When user runs the inference, below information is printed in the console. The first column here is data ID for which the trace data is being dumped. The second column represents the scaling factor for the tensor. The third and the fourth columns represent the minimum and maximum values in the tensor, respectively.The fifth column is the data element type of the tensor.</li>
</ul>
<div class="image">
<img src="trace_console.PNG" alt="trace_console.PNG"/>
<div class="caption">
Feature Map Scale Console Log</div></div>
<ul>
<li>If the minimum and maximum values are in the range of the 1 to 4 then, minimum quantization error are expected for these layers. If the range is large (&gt; 32), and has big variation between layers, then it is recommended increase the activation range using the below TIDL-RT inference create time parameter: <div class="fragment"><div class="line">quantRangeExpansionFactor = 1.5</div></div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="did_tidl_debug_fm_quant"></a>
Feature Map Comparison with Reference</h1>
<ul>
<li>Even after setting above, if the quantization error is high then user needs to analyze the dumped trace. At end of above inference traces, layer level traces are generated in the ti_dl/test/trace folder.</li>
<li>The inference software generates two files for each data ID. Note that data Id can be different from layer Id, there are two ways to find this mapping, first way is to read the mapping from the *layer_info.txt file which gets generated in the same folder as the TIDL model output location. Second way is to read the output of model visualization tool (*.svg file, gets generated in the same folder as TIDL model output location), here this information can be read from each layer box inside the square brackets [layerIdx, dataIdx].</li>
</ul>
<div class="image">
<img src="fm_trace_files.png" alt="fm_trace_files.png"/>
<div class="caption">
Feature Map Trace files</div></div>
<ul>
<li>These files can be viewed using generic binary file viewers <div class="image">
<img src="fm_float_view.PNG" alt="fm_float_view.PNG"/>
<div class="caption">
Feature Map Trace Float view</div></div>
</li>
<li>User can generate these traces for floating/16-bit and 8-bit settings in import configuration file and compare them using below sample scripts.</li>
</ul>
<h2>Script 1 : Layer level activation comparisons :</h2>
<ul>
<li>Below script can be used to generate activation comparison plots for all the layers of the network of TIDL-RT floating point output vs TIDL-RT fixed point ( 8 bit output)</li>
<li><p class="startli">Usage : python script_name.py &ndash;im &lt;import_config_file_name&gt; &ndash;in &lt;infer_config_file_name&gt;</p>
<p class="startli">e.g. python script_name.py &ndash;im ../../test/testvecs/config/import/public/caffe/tidl_import_jacintonet11v2.txt &ndash;in testvecs/config/infer/public/caffe/tidl_infer_jacintonet11v2.txt</p>
</li>
<li>This script generates floating point comparison plot by running TIDL-RT in float mode and then running the TIDL-RT inference in 8 bit mode. Currently it generates following 3 plots :<ul>
<li>Plot 1 : Histogram of difference</li>
<li>Plot 2 : TIDL fixed point inference output plotted against TIDL Floating inference output</li>
<li>Plot 3 : TIDL Floating inference output and TIDL fixed point inference output plotted in the same plot</li>
</ul>
</li>
<li>Typically no single plot is enough to conclude the difference and each gives certain way to compare the two outputs.</li>
<li>Plots will be generated in the same folder as the script inside comparison_output/activations</li>
<li>A sample plot is as shown as below : <div class="image">
<img src="sample_activation_plots.png" alt="sample_activation_plots.png"/>
<div class="caption">
Feature Map Activation comparison output</div></div>
</li>
<li>Script to generate these plots for reference :</li>
</ul>
<div class="fragment"><div class="line">import numpy as np</div><div class="line">import argparse</div><div class="line">import matplotlib</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import os</div><div class="line">import sys</div><div class="line">import subprocess</div><div class="line">import shutil</div><div class="line"></div><div class="line">debug = 0</div><div class="line"></div><div class="line">parser = argparse.ArgumentParser()</div><div class="line"></div><div class="line">parser.add_argument(&#39;-im&#39;, &#39;--import_cfg&#39;,</div><div class="line">                    default=&#39;../../test/testvecs/config/import/public/caffe/tidl_import_jacintonet11v2.txt&#39;)</div><div class="line">parser.add_argument(&#39;-in&#39;, &#39;--infer_cfg&#39;, default=&#39;testvecs/config/infer/public/caffe/tidl_infer_jacintonet11v2.txt&#39;)</div><div class="line"># TO DO Yet add support</div><div class="line">parser.add_argument(&#39;-p&#39;, &#39;--precision&#39;, default=&#39;8bit&#39;)</div><div class="line">parser.add_argument(&#39;-o&#39;, &#39;--outdir&#39;, default=&#39;comparison_output&#39;)</div><div class="line"># parser.add_argument(&#39;-l&#39;, &#39;--list_file&#39;,  default=&#39;testvecs/config/config_accuracy_list.txt&#39;)</div><div class="line">args = parser.parse_args()</div><div class="line"></div><div class="line">numParamBits = {</div><div class="line">    &quot;8bit&quot;: [&#39;8&#39;],</div><div class="line">    &quot;16bit&quot;: [&#39;16&#39;]</div><div class="line">}</div><div class="line"></div><div class="line">def save_error_plot(float_output, fixed_output, axes):</div><div class="line">    mx = np.max(float_data)</div><div class="line">    mn = np.min(float_data)</div><div class="line">    org_diff = (fixed_data - float_data)</div><div class="line">    combined = np.vstack((float_data, fixed_data, org_diff)).T</div><div class="line">    # #np.savetxt(&quot;figs\\&quot;+str(i).zfill(4)+&quot;_float.txt&quot;, combined, fmt=&#39;%10.6f, %10.6f, %10.6f&#39;)</div><div class="line">    abs_diff = abs(fixed_data - float_data)</div><div class="line">    maxIndex = np.argmax(abs_diff)</div><div class="line">    max_abs_diff = np.max(abs_diff)</div><div class="line">    mean_abs_diff = np.mean(abs_diff)</div><div class="line">    var_abs_diff = np.var(abs_diff)</div><div class="line"></div><div class="line">    axes.hist(abs_diff, color=&#39;blue&#39;, edgecolor=&#39;black&#39;, bins=60)</div><div class="line">    # image_txt = &quot;mean = &quot; + str(mean) +&quot;, Var = &quot;+ str(var) +&quot;, MAx = &quot;+ str(mx)</div><div class="line">    image_txt = &quot;MeanAbsDiff=%7.4f, MaxAbsDiff=%7.4f, MaxVal=%7.3f&quot; % (mean_abs_diff, max_abs_diff, mx)</div><div class="line">    #plt.title(image_txt)</div><div class="line">    axes.set_title(image_txt, fontdict = {&#39;fontsize&#39; : 8})</div><div class="line"></div><div class="line"></div><div class="line">def save_pc_ref_plot(float_output, fixed_output, axes):</div><div class="line">    axes.set_title(&quot;Float output Vs Fixed Output : Plot 1&quot;)</div><div class="line">    axes.set_xlabel(&#39;Float Output&#39;)</div><div class="line">    axes.set_ylabel(&#39;Fixed Output&#39;)</div><div class="line">    axes.plot(float_output, fixed_output, &#39;.&#39;)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def save_pc_ref_plot2(float_output, fixed_output, axes):</div><div class="line">    axes.set_title(&quot;Float output Vs Fixed Output : Plot 2&quot;)</div><div class="line">    axes.plot(float_output, &quot;bs&quot;, label = &quot;Float&quot;)</div><div class="line">    axes.plot(fixed_output, &quot;c.&quot;, label = &quot;Fixed&quot;)</div><div class="line">    axes.legend(loc=&#39;upper right&#39;, frameon=True)</div><div class="line"></div><div class="line"></div><div class="line">current_dir = os.getcwd();</div><div class="line">tidl_dir = os.path.abspath(os.path.join(current_dir, &quot;../../../&quot;))</div><div class="line">import_dir = os.path.join(tidl_dir, &quot;ti_dl/utils/tidlModelImport&quot;)</div><div class="line">infer_dir = os.path.join(tidl_dir, &quot;ti_dl/test&quot;)</div><div class="line">output_directory = os.path.join(current_dir, args.outdir)</div><div class="line">float_dir = os.path.join(current_dir, &quot;out_float&quot;);</div><div class="line">float_dir = os.path.join(float_dir, &quot;&quot;);</div><div class="line">fixed_dir = os.path.join(current_dir, &quot;out_8bit&quot;);</div><div class="line">fixed_dir = os.path.join(fixed_dir, &quot;&quot;);</div><div class="line"></div><div class="line">if sys.platform == &#39;win32&#39;:</div><div class="line">    import_tool = &#39;out/tidl_model_import.out.exe&#39;</div><div class="line">    tb_tool = &#39;PC_dsp_test_dl_algo.out.exe&#39;</div><div class="line">    comment_lead = &#39;::&#39;</div><div class="line">    shell_attribute = False</div><div class="line">elif sys.platform == &#39;linux&#39;:</div><div class="line">    import_tool = &#39;./out/tidl_model_import.out&#39;</div><div class="line">    tb_tool = &#39;./PC_dsp_test_dl_algo.out&#39;</div><div class="line">    comment_lead = &#39;#&#39;</div><div class="line">    shell_attribute = True</div><div class="line">else:</div><div class="line">    msg(0, &#39;Unrecognised system: %s&#39; % sys.platform)</div><div class="line">    sys.exit(1)</div><div class="line"></div><div class="line">if debug == 0 :</div><div class="line">    if os.path.exists(os.path.join(current_dir, &quot;out_8bit&quot;)):</div><div class="line">        shutil.rmtree(os.path.join(current_dir, &quot;out_8bit&quot;))</div><div class="line"></div><div class="line">    if os.path.exists(os.path.join(current_dir, &quot;out_float&quot;)):</div><div class="line">        shutil.rmtree(os.path.join(current_dir, &quot;out_float&quot;))</div><div class="line"></div><div class="line">    if os.path.exists(output_directory):</div><div class="line">        shutil.rmtree(output_directory)</div><div class="line"></div><div class="line">    os.mkdir(&quot;out_8bit&quot;)</div><div class="line">    os.mkdir(&quot;out_float&quot;)</div><div class="line">    os.mkdir(output_directory)</div><div class="line">    os.mkdir(os.path.join(output_directory, &quot;activations&quot;))</div><div class="line"></div><div class="line"></div><div class="line">    # First do the import in floating point</div><div class="line">    os.chdir(import_dir)</div><div class="line">    print(&quot;Floating Point import : Start&quot;)</div><div class="line">    import_command_base = import_tool + &quot; &quot; + args.import_cfg;</div><div class="line">    import_command_float = import_command_base + &quot; --numParamBits 32&quot;</div><div class="line">    subprocess.call(import_command_float,shell=shell_attribute)</div><div class="line">    print(&quot;Floating Point import : End&quot;)</div><div class="line"></div><div class="line">    # Inference to collect layer level traces of floating point</div><div class="line">    os.chdir(infer_dir)</div><div class="line">    print(&quot;Floating Point inference : Start&quot;)</div><div class="line">    infer_command_base = tb_tool + &quot; s:&quot; + args.infer_cfg + &quot; --writeTraceLevel 3 --debugTraceLevel 1 &quot;</div><div class="line">    infer_config_float = infer_command_base + &quot; --flowCtrl 1 &quot; + &quot;--traceDumpBaseName &quot; + float_dir</div><div class="line">    print(infer_config_float)</div><div class="line">    subprocess.call(infer_config_float,shell=shell_attribute)</div><div class="line">    print(&quot;Floating Point inference : End&quot;)</div><div class="line"></div><div class="line">    # Delete other traces other than floating point</div><div class="line">    for item in os.listdir(float_dir):</div><div class="line">        if item.endswith(&quot;.y&quot;):</div><div class="line">            os.remove(os.path.join(float_dir, item))</div><div class="line"></div><div class="line">    os.chdir(import_dir)</div><div class="line">    import_command_fixed = import_command_base + &quot; --numParamBits &quot; + numParamBits[args.precision][0]</div><div class="line">    print(import_command_fixed)</div><div class="line">    subprocess.call(import_command_fixed,shell=shell_attribute)</div><div class="line"></div><div class="line">    os.chdir(infer_dir)</div><div class="line">    infer_config_fixed = infer_command_base + &quot;--traceDumpBaseName &quot; + fixed_dir</div><div class="line">    print(infer_config_fixed)</div><div class="line">    subprocess.call(infer_config_fixed,shell=shell_attribute)</div><div class="line"></div><div class="line">    for item in os.listdir(fixed_dir):</div><div class="line">        if item.endswith(&quot;.y&quot;):</div><div class="line">            os.remove(os.path.join(fixed_dir, item))</div><div class="line"></div><div class="line">i = 0</div><div class="line"></div><div class="line">for item in os.listdir(float_dir):</div><div class="line">    fileHandle = open(os.path.join(fixed_dir, item), &#39;rb&#39;)</div><div class="line">    fixed_data = np.fromfile(fileHandle, dtype=np.float32)</div><div class="line">    fileHandle.close()</div><div class="line"></div><div class="line">    fileHandle = open(os.path.join(float_dir, item), &#39;rb&#39;)</div><div class="line">    float_data = np.fromfile(fileHandle, dtype=np.float32)</div><div class="line">    fileHandle.close()</div><div class="line"></div><div class="line">    fig = plt.figure(figsize=(10, 10))</div><div class="line">    axes1 = plt.subplot2grid((16, 16), (0, 0), rowspan=6, colspan=7)</div><div class="line">    axes2 = plt.subplot2grid((16, 16), (0, 8), rowspan=6, colspan=8)</div><div class="line">    axes3 = plt.subplot2grid((16, 16), (8, 0), rowspan=8, colspan=16)</div><div class="line"></div><div class="line">    save_error_plot(float_data, fixed_data, axes1)</div><div class="line">    save_pc_ref_plot(float_data, fixed_data, axes2)</div><div class="line">    save_pc_ref_plot2(float_data, fixed_data, axes3)</div><div class="line"></div><div class="line">    filename = os.path.join(os.path.join(output_directory, &quot;activations&quot;), str(i).zfill(4) + &quot;_activations.png&quot;);</div><div class="line">    plt.savefig(filename)</div><div class="line">    plt.clf()</div><div class="line">    plt.close(fig)</div><div class="line">    i = i + 1</div></div><!-- fragment --><p> <b>Note : These scripts are given only for reference and may not work in all kind of environment. Validated with python 3.8</b></p>
<h2>Script 2 : Floating point comparison for a specific list of layers</h2>
<div class="fragment"><div class="line">import numpy as np</div><div class="line">import argparse</div><div class="line">import matplotlib</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">parser = argparse.ArgumentParser(description=&#39;My Arg Parser&#39;)</div><div class="line">parser.add_argument(&#39;-i&#39;, &#39;--in_file_list&#39;,                       default=&#39;trcae_files_list.txt&#39;, help=&#39;test file containinglist of files to compare&#39;, required=False)</div><div class="line">args = vars(parser.parse_args())</div><div class="line"></div><div class="line">#dir *float* /o:d /s/b</div><div class="line"></div><div class="line">def save_error_plot(list,i, mean, var, mxd, mx):</div><div class="line">    plt.hist(list, color = &#39;blue&#39;, edgecolor = &#39;black&#39;,bins=60)</div><div class="line">    #image_txt = &quot;mean = &quot; + str(mean) +&quot;, Var = &quot;+ str(var) +&quot;, MAx = &quot;+ str(mx)</div><div class="line">    image_txt = &quot;MeanAbsDiff=%7.4f, MaxAbsDiff=%7.4f, MaxVal=%7.3f&quot; %(mean, mxd, mx)</div><div class="line">    plt.title(image_txt)</div><div class="line">    plt.savefig(&quot;figs\\&quot;+str(i).zfill(4)+&quot;_abs_diff_hist.png&quot;)</div><div class="line">    plt.clf()</div><div class="line"></div><div class="line">def main():</div><div class="line">    with open(args[&#39;in_file_list&#39;]) as f:</div><div class="line">        content = f.readlines()</div><div class="line">        f.close()</div><div class="line">    print(&quot;%5s, %12s, %12s, %12s, %12s %12s, %12s, %12s&quot; %(&quot;Idx&quot;, &quot;Min&quot;, &quot;Max&quot;, &quot;max_abs_diff&quot;, &quot;max_diff_idx&quot;, &quot;mean_abs_diff&quot;,  &quot;var_abs_diff&quot;, &quot;Scale&quot;))</div><div class="line">    for i, line in enumerate(content):</div><div class="line">        values = line.split()</div><div class="line"></div><div class="line">        fileHandle = open(values[0], &#39;rb&#39;)</div><div class="line">        tidl_data = np.fromfile(fileHandle, dtype=np.float32)</div><div class="line">        fileHandle.close()</div><div class="line"></div><div class="line">        fileHandle = open(values[1], &#39;rb&#39;)</div><div class="line">        ref_data = np.fromfile(fileHandle, dtype=np.float32)</div><div class="line">        fileHandle.close()</div><div class="line"></div><div class="line">        mx = np.max(ref_data)</div><div class="line">        mn = np.min(ref_data)</div><div class="line">        org_diff = (tidl_data - ref_data)</div><div class="line">        combined = np.vstack((ref_data, tidl_data, org_diff)).T</div><div class="line">        np.savetxt(&quot;figs\\&quot;+str(i).zfill(4)+&quot;_float.txt&quot;, combined, fmt=&#39;%10.6f, %10.6f, %10.6f&#39;)</div><div class="line">        abs_diff = abs(tidl_data - ref_data)</div><div class="line">        maxIndex      = np.argmax(abs_diff)</div><div class="line">        max_abs_diff  = np.max(abs_diff)</div><div class="line">        mean_abs_diff = np.mean(abs_diff)</div><div class="line">        var_abs_diff  = np.var(abs_diff)</div><div class="line">        save_error_plot(abs_diff, i,mean_abs_diff,var_abs_diff,max_abs_diff,mx)</div><div class="line">        rng = max(np.abs(mx), np.abs(mn))</div><div class="line">        if(mn &lt; 0):</div><div class="line">            scale = 127/rng if rng!=0 else 0</div><div class="line">            tidl_data = np.round(tidl_data * scale)</div><div class="line">            tidl_data = tidl_data.astype(np.int8)</div><div class="line">        else:</div><div class="line">            scale = 255/rng if rng!=0 else 0</div><div class="line">            tidl_data = np.round(tidl_data * scale)</div><div class="line">            tidl_data = tidl_data.astype(np.uint8)</div><div class="line"></div><div class="line">        tidl_data = np.asarray(tidl_data, order=&quot;C&quot;)</div><div class="line">        with open(values[0]+&quot;viz.y&quot;,&#39;wb&#39;) as file:</div><div class="line">            file.write(tidl_data)</div><div class="line">            file.close()</div><div class="line"></div><div class="line"></div><div class="line">        print(&quot;%5s, %12.5f, %12.5f, %12.5f, %12d, %12.5f, %12.5f %12.5f&quot; %(i, mn, mx, max_abs_diff, maxIndex, mean_abs_diff,  var_abs_diff, scale))</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    main()</div></div><!-- fragment --><p>The input list file shall contain the trace file names as below:</p>
<div class="fragment"><div class="line">D:\trace_8-bit\onnx_tidl_infer_resnet18v1.txt_0001_00064_00112x00112_float.bin D:\trace_16-bit\onnx_tidl_infer_resnet18v1.txt_0001_00064_00112x00112_float.bin</div><div class="line">D:\trace_8-bit\onnx_tidl_infer_resnet18v1.txt_0002_00064_00056x00056_float.bin D:\trace_16-bit\onnx_tidl_infer_resnet18v1.txt_0002_00064_00056x00056_float.bin</div></div><!-- fragment --><ul>
<li>If user passes the file paths to 8-bit and 16-bit traces , then this scripts would write the float values and differences to a text files like below</li>
</ul>
<div class="image">
<img src="fm_float_numpy_view.PNG" alt="fm_float_numpy_view.PNG"/>
<div class="caption">
Feature Map Trace Float view - Using numPy</div></div>
<ul>
<li>This script also generates below histogram for each tensor. If the mean difference is close to maximum difference then we have, then this particular tensor has higher quantization loss.</li>
</ul>
<div class="image">
<img src="0001_abs_diff_hist.png" alt="0001_abs_diff_hist.png"/>
<div class="caption">
FM Difference Histogram</div></div>
<p> <b>Note : These scripts are given only for reference and may not work in all kind of environtment. Validated with python 3.8</b></p>
<h1>Reference Traces from Frameworks</h1>
<ul>
<li>If the TIDL-RT floating point inference (numParamBits = 32) is also not providing expected accuracy even after matching the pre-processing as described in <a href="md_tidl_fsg_steps_to_debug_mismatch.html#did_tidl_debug_fimage_norm_1">Here</a> and <a href="md_tidl_fsg_steps_to_debug_mismatch.html#did_tidl_debug_fimage_norm_2">Here</a>, then there is possibility that some of the layers have very high quantization loss or functionally not working as expected. To find such layers, user need to generate the layer level floating point traces from training frameworks/ PC Runtime codes like Caffe, TensorFlow, MXNet etc. and compare against TIDL-RT floating point inference output.</li>
<li>Since TIDL-RT merges many layers during import of the model, user need know the tensor name in original model corresponding to each data ID in the TIDL model. User can refer the "*_netLog.txt" in the same path as imported model</li>
<li>The outDataNames, here corresponds to the tensor name in the original network. User can use this information and one of the reference scripts below to generate the trace information.</li>
</ul>
<div class="image">
<img src="Debug_Net_log.PNG" alt="Debug_Net_log.PNG"/>
<div class="caption">
Network Log</div></div>
<ul>
<li>All the scripts provided below are for reference purpose only. These may not directly work with any version. User may need to update scripts as per the latest API in the corresponding framework by taking help from community forums.</li>
</ul>
<h2><a class="anchor" id="did_tidl_debug_caffe_infer"></a>
Example script to Reference output from Caffe</h2>
<div class="fragment"><div class="line">import os</div><div class="line">import os.path</div><div class="line">import time</div><div class="line">import sys</div><div class="line">import ntpath</div><div class="line"></div><div class="line">model_path = &quot;/mnt/d/work/vision/c7x-mma-tidl/ti_dl/test/testvecs/models/public/caffe/resNet10/deploy.prototxt&quot;</div><div class="line">pretrained_path = &quot;/mnt/d/work/vision/c7x-mma-tidl/ti_dl/test/testvecs/models/public/caffe/resNet10/resnet10_cvgj_iter_320000.caffemodel&quot;</div><div class="line">input_name = &quot;/mnt/d/work/vision/c7x-mma-tidl/ti_dl/test/testvecs/input/airshow.jpg&quot;</div><div class="line"></div><div class="line">import caffe</div><div class="line">caffe.set_mode_cpu()</div><div class="line">from caffe.proto import caffe_pb2</div><div class="line">import cv2</div><div class="line">import numpy as np</div><div class="line">import math</div><div class="line">import string</div><div class="line">from google.protobuf import text_format</div><div class="line"></div><div class="line">def writeNPAryAsRaw(ipFrame, fileName, opDataType=np.float32, opScale=1):</div><div class="line">    if opDataType != np.float32:</div><div class="line">        qFrame = np.rint(ipFrame * opScale)</div><div class="line">    else:</div><div class="line">        qFrame = ipFrame</div><div class="line"></div><div class="line">    fileHandle = open(fileName, &#39;wb&#39;)</div><div class="line">    ip1DAry = np.reshape(qFrame, (1, np.prod(qFrame.shape)))</div><div class="line">    ip1DAry = ip1DAry.astype(opDataType)</div><div class="line">    fileHandle.write(ip1DAry)</div><div class="line">    fileHandle.close()</div><div class="line"></div><div class="line">def predict(model_path, pretrained_path, image, frameNum, blobs=None):</div><div class="line">    net = caffe.Net(model_path, pretrained_path, caffe.TEST)</div><div class="line"></div><div class="line">    input_dims = net.blobs[&#39;data&#39;].shape</div><div class="line">    print (&quot;input dim from desc&quot;, input_dims[2], input_dims[3])</div><div class="line"></div><div class="line">    batch_size, num_channels, input_height, input_width = input_dims</div><div class="line">    caffe_in = np.zeros(input_dims, dtype=np.float32)</div><div class="line"></div><div class="line">    caffe_in[0] = image.transpose([2, 0, 1])</div><div class="line">    out_blobs = net.forward_all(blobs, **{net.inputs[0]: caffe_in})</div><div class="line"></div><div class="line">    return out_blobs, net</div><div class="line"></div><div class="line">def getLayerByName(net_proto, layer_name):</div><div class="line">    for layer in net_proto.layer:</div><div class="line">    if layer.name == layer_name:</div><div class="line">        return layer</div><div class="line">    return None</div><div class="line"></div><div class="line">def infer():</div><div class="line">    caffe.set_mode_cpu()</div><div class="line">    mean_pixel = [0, 0, 0]</div><div class="line">    num = 0</div><div class="line">    use_cur_scale = True</div><div class="line"></div><div class="line">    net_proto = caffe_pb2.NetParameter()</div><div class="line">    text_format.Merge(open(model_path).read(), net_proto)</div><div class="line"></div><div class="line">    # moved image reading out from predict()</div><div class="line">    image = cv2.imread(input_name, 1);</div><div class="line">    image = cv2.resize(image, (224, 224))</div><div class="line"></div><div class="line">    image = image.astype(np.float32)- mean_pixel</div><div class="line">    layer_names=[&#39;prob&#39;, &#39;data&#39;,  &#39;data_scale&#39;, &#39;conv1_relu&#39;, &#39;layer_64_1_relu2&#39;,&#39;layer_64_1_conv2&#39;,&#39;layer_64_1_sum&#39;,&#39;layer_128_1_relu1&#39;,&#39;layer_128_1_relu2&#39;]</div><div class="line">    blob_names =[&#39;prob&#39;, &#39;data&#39;,  &#39;data_bn&#39;, &#39;conv1&#39;,&#39;layer_64_1_conv1&#39;,&#39;layer_64_1_conv2&#39;, &#39;layer_64_1_sum&#39;,&#39;layer_128_1_bn1&#39;, &#39;layer_128_1_conv1&#39;]</div><div class="line">    blob_type  =[&#39;int8&#39;, &#39;uint8&#39;, &#39;int8&#39;, &#39;uint8&#39;, &#39;uint8&#39;, &#39;int8&#39;, &#39;int8&#39;, &#39;uint8&#39;, &#39;uint8&#39;]</div><div class="line">    out_blobs, net = predict(model_path, pretrained_path, image, num, blobs=blob_names)</div><div class="line"></div><div class="line">    dataOut = out_blobs[&#39;prob&#39;]</div><div class="line">    print(dataOut.shape)</div><div class="line">    argIndex = np.argsort(np.squeeze(dataOut))[::-1][:10]</div><div class="line"></div><div class="line">    print (argIndex)</div><div class="line">    if &#39;data&#39; in out_blobs.keys():</div><div class="line">        writeNPAryAsRaw(out_blobs[&#39;data&#39;], &#39;data&#39;+&#39;_orgIn&#39;+&#39;.y&#39;, opDataType=np.uint8, opScale=1)</div><div class="line"></div><div class="line">    for blobName in out_blobs.keys():</div><div class="line">    layerIndex = blob_names.index(blobName)</div><div class="line">    layerName = layer_names[layerIndex]</div><div class="line">    print(layerName, blobName)</div><div class="line">    layerParam =  getLayerByName(net_proto, layerName)</div><div class="line">    min_val = np.min(out_blobs[blobName])</div><div class="line">    max_val = np.max(out_blobs[blobName])</div><div class="line"></div><div class="line">    elementType = blob_type[layerIndex]</div><div class="line">    scale   = 255/np.abs(max_val) if elementType==&#39;uint8&#39; else 127/np.maximum(np.abs(max_val), np.abs(min_val))</div><div class="line"></div><div class="line">    print(scale)</div><div class="line">    writeNPAryAsRaw(out_blobs[blobName], blobName+&#39;_float32&#39;+&#39;.y&#39;, opDataType=np.float32)</div><div class="line">    if elementType==&#39;int8&#39;:</div><div class="line">        out_blobs[blobName] = out_blobs[blobName] + 128/scale</div><div class="line">        writeNPAryAsRaw(out_blobs[blobName], blobName+&#39;_&#39;+elementType+&#39;.y&#39;, opDataType=np.int8, opScale=scale)</div><div class="line">    else :</div><div class="line">        writeNPAryAsRaw(out_blobs[blobName], blobName+&#39;_&#39;+elementType+&#39;.y&#39;, opDataType=np.uint8, opScale=scale)</div><div class="line"></div><div class="line">def main():</div><div class="line">    infer()</div><div class="line"></div><div class="line">if __name__ == &#39;__main__&#39;:</div><div class="line">    main()</div></div><!-- fragment --><h2><a class="anchor" id="did_tidl_debug_onnx_mxnet_infer"></a>
Example script to Reference output from MxNet for ONNX</h2>
<div class="fragment"><div class="line">import mxnet as mx</div><div class="line">import numpy as np</div><div class="line">from collections import namedtuple</div><div class="line">from mxnet.gluon.data.vision import transforms</div><div class="line">from mxnet.contrib.onnx.onnx2mx.import_model import import_model</div><div class="line">import os</div><div class="line">import argparse</div><div class="line"></div><div class="line">parser = argparse.ArgumentParser(description=&#39;My Arg Parser&#39;)</div><div class="line">parser.add_argument(&#39;-m&#39;, &#39;--model&#39;,                       default=&#39;../../test/testvecs/models/public/onnx/squeezenet1.1.onnx&#39;, help=&#39;Input Onnx model to load&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-i&#39;, &#39;--image&#39;,                       default=&#39;../../test/testvecs/input/airshow.jpg&#39;, help=&#39;Input Image to infer&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-t&#39;, &#39;--trace_enable&#39;,  type=int,     default=1, help=&#39;Set 1 to enable trace&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-d&#39;, &#39;--dir_trace&#39;,                   default=&#39;trace/&#39;, help=&#39;Base Directory for trace&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-b&#39;, &#39;--input_tensor_name&#39;,           default=&#39;data&#39;, help=&#39;Input tensor name&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-r&#39;, &#39;--rgb_input&#39;,     type=int,     default=1, help=&#39;Input tensor RGB or BGR, set to 1 for RGB and 0 for BGR&#39;, required=False)</div><div class="line">parser.add_argument(&#39;-p&#39;, &#39;--pre_proc_type&#39;,     type=int, default=0, help=&#39;Pre-Processing type for Input&#39;, required=False)</div><div class="line">args = vars(parser.parse_args())</div><div class="line"></div><div class="line">print(args[&#39;model&#39;])</div><div class="line"></div><div class="line">#mx.test_utils.download(&#39;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&#39;)</div><div class="line">#mx.test_utils.download(&#39;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&#39;)</div><div class="line">with open(&#39;synset.txt&#39;, &#39;r&#39;) as f:</div><div class="line">    labels = [l.rstrip() for l in f]</div><div class="line"></div><div class="line"># Enter path to the ONNX model file</div><div class="line">model_path= args[&#39;model&#39;]</div><div class="line">sym, arg_params, aux_params = import_model(model_path)</div><div class="line"></div><div class="line"></div><div class="line">Batch = namedtuple(&#39;Batch&#39;, [&#39;data&#39;])</div><div class="line">def get_image(path, show=False):</div><div class="line">    img = mx.image.imread(path, to_rgb=args[&#39;rgb_input&#39;])</div><div class="line">    if img is None:</div><div class="line">        return None</div><div class="line">    return img</div><div class="line"></div><div class="line"></div><div class="line">def preprocess(img):</div><div class="line">    transform_fn = transforms.Compose([</div><div class="line">    transforms.Resize(256),</div><div class="line">    transforms.CenterCrop(224),</div><div class="line">    transforms.ToTensor(),</div><div class="line">    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</div><div class="line">    ])</div><div class="line">    img = transform_fn(img)</div><div class="line">    img = img.expand_dims(axis=0)</div><div class="line">    return img</div><div class="line"></div><div class="line">def preprocess_1(img):</div><div class="line">    transform_fn = transforms.Compose([</div><div class="line">    transforms.Resize(224, keep_ratio=True),</div><div class="line">    transforms.CenterCrop(224),</div><div class="line">    transforms.ToTensor(),</div><div class="line">    transforms.Normalize([0.5, 0.5, 0.5], [0.00390625, 0.00390625, 0.00390625])</div><div class="line">    ])</div><div class="line">    img = transform_fn(img)</div><div class="line">    img = img.expand_dims(axis=0)</div><div class="line">    return img</div><div class="line"></div><div class="line">def predict(path):</div><div class="line">    img = get_image(path, show=False)</div><div class="line">    if args[&#39;pre_proc_type&#39;] == 0 :</div><div class="line">        img = preprocess(img)</div><div class="line">    elif args[&#39;pre_proc_type&#39;] == 1 :</div><div class="line">        img = preprocess_1(img)</div><div class="line">    mod.forward(Batch([img]))</div><div class="line">    internal_dict = dict(zip(all_names, mod.get_outputs()))</div><div class="line">    #print(internal_dict[output_names[0]])</div><div class="line">    # Take softmax to generate probabilities</div><div class="line">    scores = mx.ndarray.softmax(np.squeeze(internal_dict[output_names[0]])).asnumpy()</div><div class="line">    if(args[&#39;trace_enable&#39;]):</div><div class="line">        print(&#39;Writing&#39;)</div><div class="line">        for name in all_names:</div><div class="line">            act = internal_dict[name]</div><div class="line">            act = act.asnumpy()</div><div class="line">            maxval = max(act.ravel())</div><div class="line">            minval = min(act.ravel())</div><div class="line">            rng = max(np.abs(maxval), np.abs(minval))</div><div class="line">            if minval &gt;= 0 :</div><div class="line">                scale = 255/rng if rng!=0 else 0</div><div class="line">            else :</div><div class="line">                scale = 127/rng if rng!=0 else 0</div><div class="line"></div><div class="line">            #print(name, act.shape, minval, maxval)</div><div class="line">            act_shape = act.shape</div><div class="line">            file_name = name+str(act_shape)</div><div class="line">            file_name = file_name.replace(&quot;/&quot;,&quot;_&quot;).replace(&quot;(&quot;,&quot;_&quot;).replace(&quot;)&quot;,&quot;&quot;).replace(&quot;, &quot;,&quot;x&quot;).replace(&quot;,&quot;,&quot;x&quot;).replace(&quot; &quot;,&quot;_&quot;)+&quot;.y&quot;</div><div class="line">            file_name = &quot;trace/&quot;+file_name</div><div class="line">            print(file_name, act_shape, minval, maxval, scale)</div><div class="line">            with open(file_name+&quot;float.bin&quot;,&#39;wb&#39;) as file:</div><div class="line">                act_float = act.astype(np.float32)</div><div class="line">                act_float = np.asarray(act_float, order=&quot;C&quot;)</div><div class="line">                file.write(act_float)</div><div class="line">                file.close()</div><div class="line"></div><div class="line">            file_name = name+str(act_shape[-2:])</div><div class="line">            file_name = file_name.replace(&quot;/&quot;,&quot;_&quot;).replace(&quot;(&quot;,&quot;_&quot;).replace(&quot;)&quot;,&quot;&quot;).replace(&quot;, &quot;,&quot;x&quot;).replace(&quot;,&quot;,&quot;x&quot;).replace(&quot; &quot;,&quot;_&quot;)+&quot;.y&quot;</div><div class="line">            file_name = &quot;trace/&quot;+file_name</div><div class="line">            with open(file_name,&#39;wb&#39;) as file:</div><div class="line">                #act = np.round(act * scale) + 128</div><div class="line">                act = np.round(act * scale)</div><div class="line">                act = act.astype(np.uint8)</div><div class="line">                act = np.asarray(act, order=&quot;C&quot;)</div><div class="line">                file.write(act)</div><div class="line">    # print the top-5 inferences class</div><div class="line">    scores = np.squeeze(scores)</div><div class="line">    a = np.argsort(scores)[::-1]</div><div class="line">    print(a[0:5])</div><div class="line">    for i in a[0:5]:</div><div class="line">        print(&#39;class=%s ; probability=%f&#39; %(labels[i],scores[i]))</div><div class="line"></div><div class="line"></div><div class="line"># Determine and set context</div><div class="line">if len(mx.test_utils.list_gpus())==0:</div><div class="line">    ctx = mx.cpu()</div><div class="line">else:</div><div class="line">    ctx = mx.gpu(0)</div><div class="line"></div><div class="line">output_names = sym.list_outputs()</div><div class="line">#print(&#39;Output : &#39;)</div><div class="line">#print(output_names)</div><div class="line">if args[&#39;trace_enable&#39;]:</div><div class="line">    sym = sym.get_internals()</div><div class="line">    blob_names = sym.list_outputs()</div><div class="line">    sym_group = []</div><div class="line">    for i in range(len(blob_names)):</div><div class="line">        if blob_names[i] not in arg_params:</div><div class="line">            x = sym[i]</div><div class="line">            if blob_names[i] not in output_names:</div><div class="line">                x = mx.symbol.BlockGrad(x, name=blob_names[i])</div><div class="line">            sym_group.append(x)</div><div class="line">    sym = mx.symbol.Group(sym_group)</div><div class="line"></div><div class="line">all_names = sym.list_outputs()</div><div class="line">#print(&#39;All Output : &#39;)</div><div class="line">#print(all_names)</div><div class="line"></div><div class="line"># Load module</div><div class="line">mod = mx.mod.Module(symbol=sym, data_names=(args[&#39;input_tensor_name&#39;],), context=ctx, label_names=None)</div><div class="line"></div><div class="line">mod.bind(for_training=False, data_shapes=[(args[&#39;input_tensor_name&#39;], (1,3,224,224))],</div><div class="line">        label_shapes=mod._label_shapes)</div><div class="line">mod.set_params(arg_params, aux_params, allow_missing=True, allow_extra=True)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># Enter path to the inference image below</div><div class="line">#img_path = &#39;kitten.jpg&#39;</div><div class="line">img_path = args[&#39;image&#39;]</div><div class="line"></div><div class="line">predict(img_path)</div></div><!-- fragment --><h2><a class="anchor" id="did_tidl_debug_tf_infer"></a>
Example script to Reference output from TensorFlow for Frozen Graph</h2>
<div class="fragment"><div class="line">import math</div><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">import time</div><div class="line">import os</div><div class="line">import numpy as np</div><div class="line">import PIL</div><div class="line">import PIL.Image as Image</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_string(   &#39;graph_path&#39;, &#39;tmp\keras_frozen_optimized.pb&#39;,  &#39;The directory where the graph was written to or an absolute path to a &#39;)</div><div class="line">tf.app.flags.DEFINE_string(   &#39;input_file&#39;, &#39;00010.png&#39;,  &#39;Input image to be inferred &#39;)</div><div class="line">tf.app.flags.DEFINE_integer(  &#39;infer_image_height&#39;, 32, &#39;Infer image height&#39;)</div><div class="line">tf.app.flags.DEFINE_integer(  &#39;infer_image_width&#39;, 32, &#39;Infer image width&#39;)</div><div class="line">tf.app.flags.DEFINE_bool(     &#39;write_activations&#39;, True, &#39;Write the activations to file&#39;)</div><div class="line">tf.app.flags.DEFINE_string(   &#39;input_node_names&#39;, &#39;00010.png&#39;,  &#39;Input image to be inferred &#39;)</div><div class="line">tf.app.flags.DEFINE_string(   &#39;output_node_names&#39;, &#39;00010.png&#39;,  &#39;Input image to be inferred &#39;)</div><div class="line">tf.app.flags.DEFINE_string(   &#39;preproc_type&#39;, &#39;inception&#39;,  &#39;Type of pre proc to be applied ,eg: inception or vgg &#39;)</div><div class="line"></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">def all_nodes():</div><div class="line">nodes = [n for n in tf.get_default_graph().as_graph_def().node]</div><div class="line">node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]</div><div class="line">ops = []</div><div class="line">for node in nodes:</div><div class="line">    #print(node.name, node.op)</div><div class="line">    outputs = tf.get_default_graph().get_operation_by_name(node.name).outputs</div><div class="line">    if len(outputs) &gt; 0:</div><div class="line">    op_name = outputs[0]</div><div class="line">    ops.append(op_name)</div><div class="line">return ops, nodes, node_names</div><div class="line"></div><div class="line"></div><div class="line">def infer_frozen_graph(graph_filename):</div><div class="line">ext = os.path.splitext(graph_filename)[-1]</div><div class="line"></div><div class="line">if ext == &quot;.pbtxt&quot;:</div><div class="line">    with tf.gfile.GFile(graph_filename, &#39;r&#39;) as gfilep:</div><div class="line">    graph_def = tf.GraphDef()</div><div class="line">    graph_def.ParseFromString(gfilep.read())</div><div class="line">elif ext == &quot;.pb&quot;:</div><div class="line">    with tf.gfile.GFile(graph_filename, &#39;rb&#39;) as gfilep:</div><div class="line">    graph_def = tf.GraphDef()</div><div class="line">    graph_def.ParseFromString(gfilep.read())</div><div class="line"></div><div class="line">with tf.Graph().as_default() as graph:</div><div class="line">    _ = tf.import_graph_def(graph_def, name=&#39;&#39;)</div><div class="line"></div><div class="line">    infer_image_height = FLAGS.infer_image_height</div><div class="line">    infer_image_width = FLAGS.infer_image_width</div><div class="line"></div><div class="line">    image = Image.open(FLAGS.input_file)</div><div class="line">    image = image.convert(&#39;RGB&#39;)</div><div class="line"></div><div class="line">    #image Net center crop</div><div class="line">    if FLAGS.preproc_type == &quot;inception&quot; :</div><div class="line">        image = crop_image_by_factor(image)</div><div class="line"></div><div class="line">    image = image.resize((infer_image_height,infer_image_width),resample=PIL.Image.BICUBIC)</div><div class="line">    image = np.array(image, dtype=np.float32)</div><div class="line"></div><div class="line">    if FLAGS.preproc_type == &quot;inception&quot; :</div><div class="line">        mean_array = [128, 128, 128]</div><div class="line">    elif FLAGS.preproc_type == &quot;vgg&quot; :</div><div class="line">        mean_array = [123.68, 116.78, 103.94]</div><div class="line">    else :</div><div class="line">        mean_array = [0, 0, 0]</div><div class="line"></div><div class="line"></div><div class="line">    mean = np.array(mean_array, dtype=np.float32) #Mean Substraction</div><div class="line">    mean.reshape(1, 1, 1, 3)</div><div class="line">    image = image.reshape(-1,infer_image_height,infer_image_width,3)</div><div class="line">    image = (image - mean)</div><div class="line">    with open(&quot;input_data.y&quot;,&#39;wb&#39;) as file:</div><div class="line">    inDtaUint8 = image.astype(np.int8)</div><div class="line">    inDtaUint8 = inDtaUint8.transpose([0,3,1,2])</div><div class="line">    inDtaUint8 = np.asarray(inDtaUint8, order=&quot;C&quot;)</div><div class="line">    file.write(inDtaUint8)</div><div class="line">    if FLAGS.preproc_type == &quot;inception&quot; :</div><div class="line">        image = image / 128.0</div><div class="line"></div><div class="line">    tf.logging.info(&#39;Evaluating %s&#39; % graph_filename)</div><div class="line">    with tf.Session() as sess:</div><div class="line">    input = graph.get_operation_by_name(FLAGS.input_node_names).outputs[0]</div><div class="line">    #predictions = graph.get_operation_by_name(&quot;jacintonet_v1_11_custom_stride/conv1a/Pad&quot;).outputs[0]</div><div class="line">    predictions, nodes, node_names = all_nodes()</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line">    sess.run(init)</div><div class="line">    activations = sess.run(predictions, feed_dict={input:image})</div><div class="line"></div><div class="line">    if(FLAGS.write_activations):</div><div class="line">    print(&#39;Writing&#39;)</div><div class="line">    for act, node in zip(activations, nodes):</div><div class="line">        print(node.name, act.shape)</div><div class="line">        act_shape = act.shape</div><div class="line">        file_name = node.name+str(act.shape)</div><div class="line">        file_name = file_name.replace(&quot;/&quot;,&quot;_&quot;).replace(&quot;(&quot;,&quot;_&quot;).replace(&quot;)&quot;,&quot;&quot;).replace(&quot;, &quot;,&quot;x&quot;).replace(&quot;,&quot;,&quot;x&quot;).replace(&quot; &quot;,&quot;_&quot;)+&quot;.y&quot;</div><div class="line">        file_name = &quot;trace/&quot;+file_name</div><div class="line">        mx = max(act.ravel())</div><div class="line">        mn = min(act.ravel())</div><div class="line">        rng = max(np.abs(mx), np.abs(mn))</div><div class="line">        scale = 127/rng if rng!=0 else 0</div><div class="line">        print(file_name, act_shape, mn, mx, scale)</div><div class="line">        with open(file_name+&quot;float.bin&quot;,&#39;wb&#39;) as file:</div><div class="line">            act_float = act.astype(np.float32)</div><div class="line">            if len(act.shape)==4:</div><div class="line">                act_float = act_float.transpose([0,3,1,2])</div><div class="line">            act_float = np.asarray(act_float, order=&quot;C&quot;)</div><div class="line">            file.write(act_float)</div><div class="line">            file.close()</div><div class="line"></div><div class="line">        with open(file_name,&#39;wb&#39;) as file:</div><div class="line">        if node.name!=&#39;input_ignore&#39;:</div><div class="line">            #act = np.round(act * scale) + 128</div><div class="line">            act = np.round(act * scale)</div><div class="line">        act = act.astype(np.uint8)</div><div class="line">        if len(act.shape)==4:</div><div class="line">            act = act.transpose([0,3,1,2])</div><div class="line">        act = np.asarray(act, order=&quot;C&quot;)</div><div class="line">        file.write(act)</div><div class="line">        #print(act)</div><div class="line"></div><div class="line">    idx = 0</div><div class="line">    for act, node in zip(activations, nodes):</div><div class="line">    print(node.name)</div><div class="line">    if node.name == FLAGS.output_node_names :</div><div class="line">        act_idx = idx</div><div class="line">    idx += 1</div><div class="line">    inferred_label = np.argmax(activations[act_idx])</div><div class="line">    inferred_label_top5 = np.argsort(activations[act_idx].ravel())[::-1][0:5]</div><div class="line"></div><div class="line">    print(inferred_label)</div><div class="line">    print(inferred_label_top5)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def crop_image_by_factor(image, factor=0.875):</div><div class="line">width  = image.size[0]</div><div class="line">height = image.size[1]</div><div class="line"></div><div class="line">crop_width  =  width*factor</div><div class="line">crop_height =  height*factor</div><div class="line">half_the_width = image.size[0] / 2</div><div class="line">half_the_height = image.size[1] / 2</div><div class="line"></div><div class="line">image = image.crop((half_the_width - crop_width/2,</div><div class="line">    half_the_height - crop_height/2,</div><div class="line">    half_the_width + crop_width/2,</div><div class="line">    half_the_height + crop_height/2))</div><div class="line">return image</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def main(_):</div><div class="line">    infer_frozen_graph(FLAGS.graph_path)</div><div class="line"></div><div class="line">if __name__ == &#39;__main__&#39;:</div><div class="line">tf.app.run()</div></div><!-- fragment --><h2><a class="anchor" id="did_tidl_debug_tflite_infer"></a>
Example script to Reference output from TensorFlow for TFLite model</h2>
<div class="fragment"><div class="line">from __future__ import absolute_import, division, print_function</div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line">import PIL</div><div class="line">import PIL.Image as Image</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">tflite_mnist_model = &#39;2019-08-15_20-04-59unet_model.tflite&#39;</div><div class="line">interpreter = tf.lite.Interpreter(model_path=tflite_mnist_model)</div><div class="line">interpreter.allocate_tensors()</div><div class="line"></div><div class="line">print(&quot;\nInput Tensors&quot;)</div><div class="line">print(interpreter.get_input_details())</div><div class="line">print(&quot;\nOutput Tensors&quot;)</div><div class="line">print(interpreter.get_output_details())</div><div class="line"></div><div class="line">input_details = interpreter.get_input_details()</div><div class="line">output_details = interpreter.get_output_details()</div><div class="line">input_shape = input_details[0][&#39;shape&#39;]</div><div class="line"></div><div class="line">image = Image.open(&#39;.images/american_bulldog_124.jpg&#39;)</div><div class="line">image = image.convert(&#39;RGB&#39;)</div><div class="line">image = image.resize((input_shape[2],input_shape[1]),resample=PIL.Image.BICUBIC)</div><div class="line">image = np.array(image, dtype=np.float32)</div><div class="line">image = image.reshape(-1,input_shape[2],input_shape[1],3)</div><div class="line">image = image/128.0 - 1</div><div class="line"></div><div class="line">interpreter.set_tensor(input_details[0][&#39;index&#39;], image)</div><div class="line">interpreter.invoke()</div><div class="line">output_data = interpreter.get_tensor(output_details[0][&#39;index&#39;])</div><div class="line">pred_mask = output_data</div><div class="line">pred_mask = pred_mask[..., tf.newaxis]</div></div><!-- fragment --><ul>
<li>In case of TF lite user can not take generate output of any intermediate layers traces. Only final output layers can be predicted using TFLite model</li>
<li>For getting layer level traces, user need to make TFLite model by making intermediate layers as output.</li>
<li>User can refer code to make a ny layer in keras model as output layer</li>
</ul>
<div class="fragment"><div class="line">outputs = [model.get_layer(&quot;conv2d_4&quot;).output, model.get_layer(&quot;conv2d_5&quot;).output]</div><div class="line">model_infer = tf.keras.Model(inputs=model.inputs, outputs=outputs)</div></div><!-- fragment --> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
